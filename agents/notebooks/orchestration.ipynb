{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c29f55ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure project root is on sys.path for absolute imports\n",
        "import os, sys\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3044d229",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8be0830e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from agents.rag_ingest import initialize_vectorstore_with_rag_chain\n",
        "from pathlib import Path\n",
        "from tempfile import mkdtemp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "69c3a270",
      "metadata": {},
      "outputs": [],
      "source": [
        "FILE_PATH = \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\n",
        "TOP_K = 3\n",
        "milvus_uri = str(Path(mkdtemp()) / \"vector.db\")\n",
        "collection_name=\"vectordb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c44555f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-01-09 01:11:59,374 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2026-01-09 01:11:59,427 - INFO - Going to convert document batch...\n",
            "2026-01-09 01:11:59,432 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
            "2026-01-09 01:11:59,448 - INFO - Auto OCR model selected ocrmac.\n",
            "2026-01-09 01:11:59,456 - INFO - Accelerator device: 'mps'\n"
          ]
        }
      ],
      "source": [
        "rag_chain = initialize_vectorstore_with_rag_chain(\n",
        "    FILE_PATH=FILE_PATH,\n",
        "    TOP_K=TOP_K,\n",
        "    milvus_uri=milvus_uri,\n",
        "    collection_name=collection_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c45a9f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from agents.retrieval_orchestrator_agent import create_retrieval_orchestrator_agent, Context\n",
        "from agents.resoning_agent import create_reasoning_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a271225",
      "metadata": {},
      "outputs": [],
      "source": [
        "retrieval_orchestrator_agent = create_retrieval_orchestrator_agent()\n",
        "reasoning_agent = create_reasoning_agent()\n",
        "\n",
        "# Load reasoning prompt for dynamic formatting\n",
        "def read_markdown_file(filepath):\n",
        "    \"\"\"Reads the content of a Markdown file as a string.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "        return text\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: The file at {filepath} was not found.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "reasoning_prompt = read_markdown_file(\"../prompts/reasoning_prompt.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29135202",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.types import Command\n",
        "from typing import Annotated, Any\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be22bd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MainState(MessagesState):\n",
        "    question: Annotated[str, \"The user's question\"]\n",
        "    retrieval_results: Annotated[dict | None, \"The retrieval results from the retrieval orchestrator agent\"] = None\n",
        "    final_answer: Annotated[str | None, \"The final answer generated by the reasoning agent\"] = None\n",
        "    human_approval: Annotated[bool | None, \"Whether the human approved the final answer\"] = None\n",
        "    rag_chain: Annotated[Any, \"The RAG chain to use for retrieval\"]\n",
        "    pending_review: Annotated[Any | None, \"HITL review configs returned on interrupt\"] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e28ebfb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def invoke_retrieval_orchestration(state: MainState):\n",
        "    \"\"\"Invoke the retrieval orchestrator agent to get context from RAG.\"\"\"\n",
        "    # Get the user question from state or last message\n",
        "    user_question = state.get(\"question\") or (state.get(\"messages\", [])[-1].content if state.get(\"messages\") else None)\n",
        "    if not user_question:\n",
        "        raise ValueError(\"No question found in state\")\n",
        "    \n",
        "    # Get rag_chain from state\n",
        "    rag_chain = state.get(\"rag_chain\")\n",
        "    if not rag_chain:\n",
        "        raise ValueError(\"No rag_chain found in state\")\n",
        "    \n",
        "    result = retrieval_orchestrator_agent.invoke(\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": user_question}\n",
        "            ],\n",
        "        },\n",
        "        context=Context(rag_chain=rag_chain)\n",
        "    )\n",
        "    res_messages = result[\"messages\"]\n",
        "    retrieval_results = result[\"structured_response\"][\"results\"]\n",
        "    return {\n",
        "        \"messages\": res_messages, \n",
        "        \"retrieval_results\": retrieval_results,\n",
        "        \"question\": user_question  # Ensure question is set in state\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6104c191",
      "metadata": {},
      "outputs": [],
      "source": [
        "def invoke_reasoning(state: MainState):\n",
        "    \"\"\"Invoke the reasoning agent with retrieval results.\"\"\"\n",
        "    # Get question and retrieval results from state\n",
        "    user_question = state.get(\"question\") or (state.get(\"messages\", [])[-1].content if state.get(\"messages\") else None)\n",
        "    if not user_question:\n",
        "        raise ValueError(\"No question found in state\")\n",
        "    \n",
        "    retrieval_results = state.get(\"retrieval_results\")\n",
        "    if not retrieval_results:\n",
        "        raise ValueError(\"No retrieval results found in state\")\n",
        "    \n",
        "    # Format the reasoning prompt with user question and context\n",
        "    formatted_prompt = reasoning_prompt.format(\n",
        "        user_question=user_question, \n",
        "        context=json.dumps(retrieval_results, indent=2)\n",
        "    )\n",
        "    \n",
        "    # Create messages with system prompt and user question\n",
        "    messages = [\n",
        "        SystemMessage(content=formatted_prompt),\n",
        "        HumanMessage(content=user_question)\n",
        "    ]\n",
        "    \n",
        "    cfg = {\"configurable\": {\"thread_id\": \"reasoning-thread\"}}\n",
        "\n",
        "    result = reasoning_agent.invoke({\"messages\": messages}, config=cfg)\n",
        "    res_messages = result[\"messages\"]\n",
        "\n",
        "    # If the agent interrupted for HITL, store review configs; graph edges decide routing\n",
        "    if \"__interrupt__\" in result:\n",
        "        review_configs = result[\"__interrupt__\"][-1].value[\"review_configs\"]\n",
        "        return {\"messages\": res_messages, \"pending_review\": review_configs}\n",
        "\n",
        "    # Extract final answer from structured response\n",
        "    final_answer = result.get(\"structured_response\", {}).get(\"final_answer\", \"\")\n",
        "    if not final_answer:\n",
        "        # Fallback: get from last message if structured response is missing\n",
        "        final_answer = res_messages[-1].content if res_messages else \"\"\n",
        "    \n",
        "    return {\n",
        "        \"messages\": res_messages, \n",
        "        \"final_answer\": final_answer, \n",
        "        \"pending_review\": None\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d76b8438",
      "metadata": {},
      "outputs": [],
      "source": [
        "def human_review(state: MainState):\n",
        "    \"\"\"Handle human review for web search approval.\"\"\"\n",
        "    # Present state.pending_review to a human and collect decision.\n",
        "    # For now, auto-approve to demonstrate resume flow.\n",
        "    # In production, you would show the pending_review to the user and wait for input\n",
        "    pending_review = state.get(\"pending_review\")\n",
        "    print(\"Human review needed for web search approval.\")\n",
        "    print(f\"Pending review: {pending_review}\")\n",
        "    \n",
        "    decision = {\"type\": \"approve\"}  # Auto-approve for demo; replace with user input\n",
        "    cfg = {\"configurable\": {\"thread_id\": \"reasoning-thread\"}}\n",
        "\n",
        "    resumed = reasoning_agent.invoke(\n",
        "        Command(resume={\"decisions\": [decision]}),\n",
        "        config=cfg,\n",
        "    )\n",
        "\n",
        "    res_messages = resumed[\"messages\"]\n",
        "    \n",
        "    # Extract final answer from structured response\n",
        "    final_answer = resumed.get(\"structured_response\", {}).get(\"final_answer\", \"\")\n",
        "    if not final_answer:\n",
        "        # Fallback: get from last message if structured response is missing\n",
        "        final_answer = res_messages[-1].content if res_messages else \"\"\n",
        "\n",
        "    # Graph-directed: only update; builder edge sends us to END\n",
        "    return {\n",
        "        \"messages\": res_messages,\n",
        "        \"final_answer\": final_answer,\n",
        "        \"human_approval\": True,\n",
        "        \"pending_review\": None,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4199c7a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "builder = StateGraph(MainState)\n",
        "\n",
        "builder.add_node(\"invoke_retrieval_orchestration\", invoke_retrieval_orchestration)\n",
        "builder.add_node(\"invoke_reasoning\", invoke_reasoning)\n",
        "builder.add_node(\"human_review\", human_review)\n",
        "\n",
        "builder.add_edge(START, \"invoke_retrieval_orchestration\")\n",
        "builder.add_edge(\"invoke_retrieval_orchestration\", \"invoke_reasoning\")\n",
        "\n",
        "# Route based on presence of pending_review set by invoke_reasoning\n",
        "def should_review(state: MainState) -> str:\n",
        "    \"\"\"Determine if human review is needed.\"\"\"\n",
        "    pending = state.get(\"pending_review\")\n",
        "    if pending is not None:\n",
        "        return \"human_review\"\n",
        "    return \"end\"\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"invoke_reasoning\",\n",
        "    should_review,\n",
        "    {\n",
        "        \"human_review\": \"human_review\",\n",
        "        \"end\": END,\n",
        "    }\n",
        ")\n",
        "\n",
        "builder.add_edge(\"human_review\", END)\n",
        "\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48db8118",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the graph with a user question\n",
        "user_question = \"How does the transformer model work?\"\n",
        "\n",
        "# Initialize state with the question and rag_chain\n",
        "initial_state = {\n",
        "    \"messages\": [HumanMessage(content=user_question)],\n",
        "    \"question\": user_question,\n",
        "    \"rag_chain\": rag_chain,\n",
        "    \"retrieval_results\": None,\n",
        "    \"final_answer\": None,\n",
        "    \"human_approval\": None,\n",
        "    \"pending_review\": None,\n",
        "}\n",
        "\n",
        "# Invoke the graph\n",
        "config = {\"configurable\": {\"thread_id\": \"main-thread\"}}\n",
        "result = graph.invoke(initial_state, config=config)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FINAL ANSWER:\")\n",
        "print(\"=\" * 80)\n",
        "print(result[\"final_answer\"])\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
