{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9736e944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2dc8a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"\"\"\n",
    "Example 1 single information need\n",
    "Input query:\n",
    "How does dropout prevent overfitting in neural networks\n",
    "Output:\n",
    "{\"sub_queries\":[\"dropout regularization reducing neural network overfitting\"]}\n",
    "\n",
    "Example 2 two distinct information needs\n",
    "Input query:\n",
    "effects of microplastics on marine food webs and human health risks\n",
    "Output:\n",
    "{\"sub_queries\":[\"microplastics impacts on marine food webs\",\"human health risks from microplastic exposure\"]}\n",
    "\n",
    "Example 3 avoid over splitting broad impact\n",
    "Input query:\n",
    "What are the latest advancements in natural language processing and how do they impact machine learning models\n",
    "Output:\n",
    "{\"sub_queries\":[\"recent advancements in natural language processing\",\"impact of recent NLP advances on machine learning models\"]}\n",
    "\n",
    "Example 4 three distinct information needs\n",
    "Input query:\n",
    "role of gut microbiome in obesity and type 2 diabetes and dietary interventions\n",
    "Output:\n",
    "{\"sub_queries\":[\"gut microbiome links to obesity\",\"gut microbiome links to type 2 diabetes\",\"dietary interventions modulating gut microbiome in metabolic disease\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You are an academic query decomposition and refinement agent for vector retrieval.\n",
    "You will receive a query from a user which may contain single or multiple distinct information needs.\n",
    "\n",
    "Here are the examples which are done well:\n",
    "{examples}\n",
    "\n",
    "Goal:\n",
    "Return a list of refined sub-queries for retrieving relevant academic context from a vector database.\n",
    "\n",
    "Output (strict):\n",
    "- Return ONLY an object with key \"sub_queries\" containing a list of strings.\n",
    "- Each string must be a refined retrieval query.\n",
    "- Do NOT add any other keys or any extra text.\n",
    "\n",
    "Decomposition rules:\n",
    "- If the input expresses ONE coherent information need, return exactly 1 refined query.\n",
    "- If the input contains multiple distinct information needs different questions topics or aspects return 2 to 5 sub-queries.\n",
    "- Each sub-query must be atomic one concept or aspect only.\n",
    "- Avoid overlap no near-duplicate sub-queries.\n",
    "\n",
    "Refinement rules apply to every sub-query:\n",
    "- Preserve the user intent exactly do not change meaning.\n",
    "- Do NOT add new concepts not present or clearly implied.\n",
    "- Use precise academic or technical terminology.\n",
    "- Prefer noun phrases avoid questions and full sentences.\n",
    "- Remove filler words stopwords and conversational phrasing.\n",
    "- Length 5 to 12 words per sub-query max 12.\n",
    "- Do NOT use punctuation quotes Boolean operators or special syntax.\n",
    "- Do NOT include years author names or datasets unless explicitly present in the input.\n",
    "- Properly identify distinct aspects if the input is broad or vague.\n",
    "- Do NOT over-split sub-queries unnecessarily.\n",
    "- Try to keep related concepts together.\n",
    "\n",
    "Return the structured output now.\n",
    "\"\"\".format(examples=examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a71bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class SubQueries(TypedDict):\n",
    "    \"\"\"Subqueries generated from a main query\"\"\"\n",
    "    sub_queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "236d16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3630a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "agent = create_agent(\n",
    "    model, \n",
    "    response_format = SubQueries,\n",
    "    system_prompt=PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e1b2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24847fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Explain me the concept of Scaled Dot-Product Attention and Multi-Head Attention and how they are used in the Model Architecture of Transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9648d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke({\"messages\": [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": QUESTION\n",
    "    }\n",
    "]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa1b47ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['concept of scaled dot-product attention in transformers',\n",
       " 'concept of multi-head attention in transformers',\n",
       " 'role of attention in transformer architecture']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['structured_response']['sub_queries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3102e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
