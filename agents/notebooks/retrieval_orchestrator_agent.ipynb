{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a348d68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4283d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_markdown_file(filepath):\n",
    "    \"\"\"Reads the content of a Markdown file as a string.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: The file at {filepath} was not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c28c6d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sultan/Projects/citebase/agents/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from deepagents import create_deep_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc9e87",
   "metadata": {},
   "source": [
    "**RAG.**\n",
    "Define a retrieval function that the deep_agent will use as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cf2f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\n",
    "TOP_K = 3\n",
    "queries = ['mechanics of scaled dot product attention',\n",
    " 'key aspects of multi head attention']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54dd9bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 18:12:44,583 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-07 18:12:44,674 - INFO - Going to convert document batch...\n",
      "2026-01-07 18:12:44,675 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2026-01-07 18:12:44,683 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-07 18:12:44,683 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-07 18:12:44,685 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-07 18:12:44,691 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-07 18:12:44,691 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-07 18:12:44,695 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-07 18:12:45,563 - INFO - Auto OCR model selected ocrmac.\n",
      "2026-01-07 18:12:45,569 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-07 18:12:45,570 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-07 18:12:45,572 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-07 18:12:45,577 - INFO - Accelerator device: 'mps'\n",
      "2026-01-07 18:12:47,125 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-07 18:12:47,129 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-07 18:12:47,131 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-07 18:12:47,913 - INFO - Accelerator device: 'mps'\n",
      "2026-01-07 18:12:48,673 - INFO - Processing document NIPS-2017-attention-is-all-you-need-Paper.pdf\n",
      "2026-01-07 18:13:00,375 - INFO - Finished converting document NIPS-2017-attention-is-all-you-need-Paper.pdf in 19.71 sec.\n",
      "/Users/sultan/Projects/citebase/agents/.venv/lib/python3.14/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2026-01-07 18:13:03,154 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "import tiktoken\n",
    "from docling_core.transforms.chunker.tokenizer.openai import OpenAITokenizer\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenizer = OpenAITokenizer(\n",
    "    tokenizer=enc,\n",
    "    max_tokens=128 * 1024,  # set to the model's context window\n",
    ")\n",
    "\n",
    "loader = DoclingLoader(file_path=FILE_PATH, chunker=HybridChunker(tokenizer=tokenizer))\n",
    "docs = loader.load()\n",
    "\n",
    "milvus_uri = str(Path(mkdtemp()) / \"vector.db\")\n",
    "\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"vectordb\",\n",
    "    connection_args={\"uri\": milvus_uri},\n",
    "    index_params={\"index_type\": \"FLAT\"},\n",
    "    drop_old=True,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(read_markdown_file(\"../prompts/retriever_prompt.md\"))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9247c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def retrieve_from_vectorstore(queries: list[str]) -> dict:\n",
    "    \"\"\"Retrieve and generate answers from the vectorstore for a list of queries.\n",
    "    This function invokes the RAG chain for each query and returns aggregated results.\"\"\"\n",
    "    \n",
    "    resp_dict={}\n",
    "    for query in queries:\n",
    "        resp_dict[query] = rag_chain.invoke({\"input\": query})\n",
    "        \n",
    "    return resp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cf964",
   "metadata": {},
   "source": [
    "**Prepare settings for subagents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f20edef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example 1 single information need\\nInput query:\\nHow does dropout prevent overfitting in neural networ'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = read_markdown_file(\"../prompts/query_decomposition_examples.md\")\n",
    "examples[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f186b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an academic query decomposition and refinement agent for vector retrieval.\\nYou will receive a query from a user which may contain single or multiple distinct information needs.\\n\\nHere are the examples which are done well:\\nExample 1 single information need\\nInput query:\\nHow does dropout prevent overfitting in neural networks\\nOutput:\\n{\"sub_queries\":[\"dropout regularization reducing neural network overfitting\"]}\\n\\nExample 2 two distinct information needs\\nInput query:\\neffects of microplastics o'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_decomposition_prompt = read_markdown_file(\"../prompts/query_decomposition_prompt.md\")\n",
    "query_decomposition_prompt.format(examples=examples)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cd667a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_decomposition_model = \"gpt-5-mini\"\n",
    "query_decomposition_description = \"Analyzes research queries and generates optimized, non-overlapping sub-queries for vector database retrieval.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aa6f8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a vectorstore retrieval orchestration agent. Your primary responsibility is to retrieve academic context from a vector database using refined sub-queries.\\n\\nTask:\\nYou will receive one or more sub-queries from the query decomposition agent. Your role is to use the retrieve_from_vectorstore tool to fetch relevant academic papers, citations, and context for each sub-query.\\n\\nInstructions:\\n1. Accept the list of sub-queries provided by the upstream agent.\\n2. Call retrieve_from_vectorstore with '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_retrieval_prompt = read_markdown_file(\"../prompts/vectorstore_retrieval_prompt.md\")\n",
    "vectorstore_retrieval_prompt[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e1530a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_retrieval_model = \"gpt-5-nano\"\n",
    "vectorstore_retrieval_description = \"Executes batch retrieval of academic context from vector database and preserves citations and metadata for structured result aggregation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b3c35",
   "metadata": {},
   "source": [
    "**Main Deep Agent**\n",
    "- Decomposes user queries into subqueries\n",
    "- Retrieves relevant information from the vectorstore using subqueries\n",
    "- Formats and structures the retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "764383f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from deepagents.middleware.subagents import SubAgentMiddleware\n",
    "from langchain.agents.middleware import TodoListMiddleware\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"gpt-5-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9558bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "class AggregatedContext(TypedDict):\n",
    "    sub_query: Annotated[str, \"The sub-query string\"]\n",
    "    retrieved_context: Annotated[str, \"The retrieved context string\"]\n",
    "    citations: Annotated[List[str], \"List of citation identifiers\"]\n",
    "    synthesized_answer: Annotated[str, \"The synthesized answer string\"]\n",
    "\n",
    "class AggregatedContextList(TypedDict):\n",
    "    results: Annotated[List[AggregatedContext], \"List of aggregated context for each sub-query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6509636f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are the main retrieval orchestrator for academic question answering. You coordinate sub-agents for query decomposition and vectorstore retrieval, then deliver a concise, citation-backed response.\\n\\nWorkflow:\\n1) Send the user query to query_decomposition_subagent. Expect an object with key sub_queries.\\n2) If decomposition fails or returns no sub-queries, fall back to a single sub-query equal to the original user query.\\n3) Send the full sub_queries list to vectorstore_retrieval_subagent (retrie'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = read_markdown_file(\"../prompts/retrieval_orchestrator_prompt.md\")\n",
    "PROMPT[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92a5d26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Always use the write_todos tool to break down the user's academic query into the following sequential steps:\\n        \\n1. DECOMPOSE: Analyze the user's query and identify distinct information needs. Send to query_decomposition_subagent to generate refined sub-queries.\\n2. VALIDATE: Ensure the decomposed sub-queries are non-overlapping and atomic (each addresses one concept/aspect).\\n3. RETRIEVE: Pass all sub-queries to vectorstore_retrieval_subagent in a single batch to fetch relevant academic cont\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todo_list_prompt = read_markdown_file(\"../prompts/todo_list_prompt.md\")\n",
    "todo_list_prompt[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02e20205",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=PROMPT,\n",
    "    middleware=[\n",
    "        TodoListMiddleware(\n",
    "            system_prompt=todo_list_prompt,\n",
    "        ),\n",
    "        SubAgentMiddleware(\n",
    "            default_model=\"gpt-4o\",\n",
    "            default_tools=[],\n",
    "            subagents=[\n",
    "                {\n",
    "                    \"name\": \"query_decomposition_subagent\",\n",
    "                    \"description\": query_decomposition_description,\n",
    "                    \"system_prompt\": query_decomposition_prompt,\n",
    "                    \"model\": query_decomposition_model,\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"vectorstore_retrieval_subagent\",\n",
    "                    \"description\": vectorstore_retrieval_description,\n",
    "                    \"system_prompt\": vectorstore_retrieval_prompt,\n",
    "                    \"tools\": [retrieve_from_vectorstore],\n",
    "                    \"model\": vectorstore_retrieval_model,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    "    response_format=AggregatedContextList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0493c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 18:13:23,794 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:13:26,864 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:13:34,717 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:13:37,819 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:13:42,018 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:13:45,909 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:13:47,958 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:13:54,826 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:13:56,050 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:14:01,013 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:15:24,213 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:15:27,181 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:15:48,378 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 18:16:08,960 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the self attention mechanism and how does it work in transformer models?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69ebdfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sub_query': 'self attention mechanism definition purpose',\n",
       "  'retrieved_context': 'Vaswani et al., \"Attention Is All You Need\" (Transformer paper) â€” sections 2 Background, 3.2.3 Applications of Attention in our Model, and 4 Why Self-Attention describe self-attention (intra-attention) as an attention mechanism that relates different positions of a single sequence to compute a sequence representation; they motivate it by reduced path length for long-range dependencies, parallelizability, and interpretability versus RNNs/convolutions.',\n",
       "  'citations': ['Vaswani et al., 2017 (NIPS paper)'],\n",
       "  'synthesized_answer': 'Self-attention (intra-attention) relates different positions within one sequence to compute contextualized representations, reducing the path length for long-range dependencies and enabling parallel computation across positions [Vaswani et al., 2017].'},\n",
       " {'sub_query': 'self attention computation in transformer models',\n",
       "  'retrieved_context': 'Vaswani et al., \"Attention Is All You Need\" â€” sections 2 and 3.2.3 explain that in the Transformer, keys, queries and values are derived from the same layer for encoder self-attention; attention is implemented as scaled dot-product attention; multi-head attention runs parallel attention heads to capture multiple relations; decoder self-attention is masked to prevent leftward information flow and preserve autoregressiveness [Vaswani et al., 2017].',\n",
       "  'citations': ['Vaswani et al., 2017 (NIPS paper)'],\n",
       "  'synthesized_answer': 'In Transformers, self-attention computes attention weights using scaled dot-product attention between queries, keys, and values from the same source, is executed in parallel via multiple heads, and uses masking in the decoder to enforce autoregressive generation [Vaswani et al., 2017].'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"structured_response\"]['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7515d0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task\n"
     ]
    }
   ],
   "source": [
    "for tool in result['messages'][3].tool_calls:\n",
    "    print(tool['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cde49c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â”‚ ðŸ¤– Agent Execution Summary â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[1;36mâ”‚\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36mðŸ¤– Agent Execution Summary\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36mâ”‚\u001b[0m\n",
       "\u001b[1;36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">================================================================================\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "================================================================================\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 1: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">What is the self attention mechanism and how does ...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 1: \u001b[0m\u001b[1;36mWhat is the self attention mechanism and how does ...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">What is the self attention mechanism and how does it work in transformer models?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mWhat is the self attention mechanism and how does it work in transformer models?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 2: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 2: \u001b[0m\u001b[1;36mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 3: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Updated todo list to [{'content': \"DECOMPOSE: Anal...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 3: \u001b[0m\u001b[1;36mUpdated todo list to [{'content': \"DECOMPOSE: Anal...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Updated todo list to [{'content': \"DECOMPOSE: Analyze user's query and generate refined sub-queries for vector </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">retrieval.\", 'status': 'in_progress'}, {'content': 'VALIDATE: Ensure sub-queries are non-overlapping and atomic.', </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">'status': 'pending'}, {'content': 'RETRIEVE: Pass all sub-queries to vectorstore_retrieval_subagent in a single </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">batch and fetch contexts with citations.', 'status': 'pending'}, {'content': 'AGGREGATE: Organize retrieved results</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">by sub-query, preserving citation metadata.', 'status': 'pending'}, {'content': 'SYNTHESIZE: Combine answers into a</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">concise, cited final response with key points and assumptions.', 'status': 'pending'}, {'content': 'REVIEW: Verify </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">all claims are backed by retrieved citations and finalize response.', 'status': 'pending'}]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mUpdated todo list to [{'content': \"DECOMPOSE: Analyze user's query and generate refined sub-queries for vector \u001b[0m\n",
       "\u001b[2;37mretrieval.\", 'status': 'in_progress'}, {'content': 'VALIDATE: Ensure sub-queries are non-overlapping and atomic.', \u001b[0m\n",
       "\u001b[2;37m'status': 'pending'}, {'content': 'RETRIEVE: Pass all sub-queries to vectorstore_retrieval_subagent in a single \u001b[0m\n",
       "\u001b[2;37mbatch and fetch contexts with citations.', 'status': 'pending'}, {'content': 'AGGREGATE: Organize retrieved results\u001b[0m\n",
       "\u001b[2;37mby sub-query, preserving citation metadata.', 'status': 'pending'}, {'content': 'SYNTHESIZE: Combine answers into a\u001b[0m\n",
       "\u001b[2;37mconcise, cited final response with key points and assumptions.', 'status': 'pending'}, {'content': 'REVIEW: Verify \u001b[0m\n",
       "\u001b[2;37mall claims are backed by retrieved citations and finalize response.', 'status': 'pending'}]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 4: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tool Calls: task</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 4: \u001b[0m\u001b[1;36mTool Calls: task\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Tool Calls: task</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mTool Calls: task\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 5: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">{\"sub_queries\": [\"self attention mechanism definit...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 5: \u001b[0m\u001b[1;36m{\"sub_queries\": [\"self attention mechanism definit...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">{\"sub_queries\": [\"self attention mechanism definition purpose\", \"self attention computation in transformer </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">models\"]}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37m{\"sub_queries\": [\"self attention mechanism definition purpose\", \"self attention computation in transformer \u001b[0m\n",
       "\u001b[2;37mmodels\"]}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 6: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 6: \u001b[0m\u001b[1;36mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 7: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Updated todo list to [{'content': \"DECOMPOSE: Anal...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 7: \u001b[0m\u001b[1;36mUpdated todo list to [{'content': \"DECOMPOSE: Anal...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Updated todo list to [{'content': \"DECOMPOSE: Analyze user's query and generate refined sub-queries for vector </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">retrieval.\", 'status': 'completed'}, {'content': 'VALIDATE: Ensure sub-queries are non-overlapping and atomic.', </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">'status': 'in_progress'}, {'content': 'RETRIEVE: Pass all sub-queries to vectorstore_retrieval_subagent in a single</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">batch and fetch contexts with citations.', 'status': 'pending'}, {'content': 'AGGREGATE: Organize retrieved results</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">by sub-query, preserving citation metadata.', 'status': 'pending'}, {'content': 'SYNTHESIZE: Combine answers into a</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">concise, cited final response with key points and assumptions.', 'status': 'pending'}, {'content': 'REVIEW: Verify </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">all claims are backed by retrieved citations and finalize response.', 'status': 'pending'}]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mUpdated todo list to [{'content': \"DECOMPOSE: Analyze user's query and generate refined sub-queries for vector \u001b[0m\n",
       "\u001b[2;37mretrieval.\", 'status': 'completed'}, {'content': 'VALIDATE: Ensure sub-queries are non-overlapping and atomic.', \u001b[0m\n",
       "\u001b[2;37m'status': 'in_progress'}, {'content': 'RETRIEVE: Pass all sub-queries to vectorstore_retrieval_subagent in a single\u001b[0m\n",
       "\u001b[2;37mbatch and fetch contexts with citations.', 'status': 'pending'}, {'content': 'AGGREGATE: Organize retrieved results\u001b[0m\n",
       "\u001b[2;37mby sub-query, preserving citation metadata.', 'status': 'pending'}, {'content': 'SYNTHESIZE: Combine answers into a\u001b[0m\n",
       "\u001b[2;37mconcise, cited final response with key points and assumptions.', 'status': 'pending'}, {'content': 'REVIEW: Verify \u001b[0m\n",
       "\u001b[2;37mall claims are backed by retrieved citations and finalize response.', 'status': 'pending'}]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 8: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tool Calls: task</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 8: \u001b[0m\u001b[1;36mTool Calls: task\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Tool Calls: task</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mTool Calls: task\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 9: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">{</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">  \"self attention mechanism definition purpose\":...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 9: \u001b[0m\u001b[1;36m{\u001b[0m\n",
       "\u001b[1;36m  \"self attention mechanism definition purpose\":...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">{</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">  \"self attention mechanism definition purpose\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    \"retrieved_context\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"metadata\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"pk\": 463415957437808650,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"source\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"dl_meta\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"schema_name\": \"docling_core.transforms.chunker.DocMeta\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"version\": \"1.0.0\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"doc_items\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/100\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 107.691, \"t\": 549.551, \"r\": 372.606, \"b\": 540.999, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 66]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/101\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 529.509, \"r\": 505.242, \"b\": 477.003, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 364]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/102\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 470.986, \"r\": 504.003, \"b\": 429.389, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 291]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/103\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 423.372, \"r\": 504.003, \"b\": 370.865, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 446]}]}</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"headings\": [\"3.2.3 Applications of Attention in our Model\"],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"origin\": {\"mimetype\": \"application/pdf\", \"binary_hash\": 1945408574365156663, \"filename\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"}</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page_content\": \"3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">three different ways:\\n- In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">and the memory keys and values come from the output of the encoder. This allows every position in the decoder to </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequence-to-sequence models such as [31, 2, 8].\\n- The encoder contains self-attention layers. In a self-attention </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">the decoder up to and including that position. We need to prevent leftward information flow in the decoder to </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">(setting to -âˆž ) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"metadata\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"pk\": 463415957437808643,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"source\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"dl_meta\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"schema_name\": \"docling_core.transforms.chunker.DocMeta\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"version\": \"1.0.0\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"doc_items\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/13\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 107.691, \"t\": 490.609, \"r\": 505.241, \"b\": 394.784, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 825]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/14\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 108.0, \"t\": 386.947, \"r\": 505.25, \"b\": 345.668, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 393]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/15\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 108.0, \"t\": 337.831, \"r\": 505.654, \"b\": 307.461, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 217]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/16\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 107.691, \"t\": 299.625, \"r\": 505.657, \"b\": 258.346, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 373]}]}</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"headings\": [\"2 Background\"],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"origin\": {\"mimetype\": \"application/pdf\", \"binary_hash\": 1945408574365156663, \"filename\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"}</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page_content\": \"2 Background\\nThe goal of reducing sequential computation also forms the foundation of the</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">building block, computing hidden representations in parallel for all input and output positions. In these models, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">the number of operations required to relate signals from two arbitrary input or output positions grows in the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">distance between positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">with Multi-Head Attention as described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">attention mechanism relating different positions of a single sequence in order to compute a representation of the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">and have been shown to perform well on simple-language question answering and language modeling tasks [28].\\nTo the</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">to compute representations of its input and output without using sequencealigned RNNs or convolution. In the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">models such as [14, 15] and [8].\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"metadata\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"pk\": 463415957437808654,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"source\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"dl_meta\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"schema_name\": \"docling_core.transforms.chunker.DocMeta\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"version\": \"1.0.0\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"doc_items\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/120\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 6, \"bbox\": {\"l\": 106.834, \"t\": 313.106, \"r\": 505.654, \"b\": 260.918, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 434]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/121\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 6, \"bbox\": {\"l\": 108.0, \"t\": 253.08100000000002, \"r\": 504.0, \"b\": 233.62, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 184]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/122\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 6, \"bbox\": {\"l\": 107.691, \"t\": 225.784, \"r\": 504.001, \"b\": 151.77700000000004, \"coord_origin\":</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 610]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/123\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 6, \"bbox\": {\"l\": 107.641, \"t\": 143.94100000000003, \"r\": 504.345, \"b\": 69.93399999999997, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 703]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/125\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 7, \"bbox\": {\"l\": 108.0, \"t\": 716.523, \"r\": 504.001, \"b\": 697.062, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 186]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/126\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 7, \"bbox\": {\"l\": 107.641, \"t\": 689.544, \"r\": 505.248, \"b\": 604.31, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 745]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/127\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 7, \"bbox\": {\"l\": 107.641, \"t\": 596.474, \"r\": 504.004, \"b\": 555.194, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 350]}]}</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"headings\": [\"4 Why Self-Attention\"],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"origin\": {\"mimetype\": \"application/pdf\", \"binary_hash\": 1945408574365156663, \"filename\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"}</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page_content\": \"4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">representations ( x 1 , ..., x n ) to another sequence of equal length ( z 1 , ..., z n ) , with x i , z i âˆˆ R d , </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">we consider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">computation that can be parallelized, as measured by the minimum number of sequential operations required.\\nThe </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">the length of the paths forward and backward signals have to traverse in the network. The shorter these paths </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">between any combination of positions in the input and output sequences, the easier it is to learn long-range </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">dependencies [11]. Hence we also compare the maximum path length between any two input and output positions in </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">networks composed of the different layer types.\\nAs noted in Table 1, a self-attention layer connects all positions</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">with a constant number of sequentially executed operations, whereas a recurrent layer requires O ( n ) sequential </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25]</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">representations. To improve computational performance for tasks involving very long sequences, self-attention could</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">output position. This would increase the maximum path length to O ( n/r ) . We plan to investigate this approach </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">further in future work.\\nA single convolutional layer with kernel width k &lt; n does not connect all pairs of input </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">and output positions. Doing so requires a stack of O ( n/k ) convolutional layers in the case of contiguous </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">kernels, or O ( log k ( n )) in the case of dilated convolutions [15], increasing the length of the longest paths </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">by a factor of k . Separable convolutions [6], however, decrease the complexity considerably, to O ( k Â· n Â· d + n </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Â· d 2 ) . Even with k = n , however, the complexity of a separable convolution is equal to the combination of a </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\nAs side benefit, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">self-attention could yield more interpretable models. We inspect attention distributions from our models and </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    \"answer_with_citations\": \"1) Final answer: The self-attention mechanism, also known as intra-attention, relates</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">different positions of a single sequence to compute a representation of the sequence. It is used to reduce the path</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">length between long-range dependencies, making it easier to learn such dependencies, and allows for parallel </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">computation with a constant number of sequential operations [Vaswani et al., 2017] (as discussed in 3.2.3 </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Applications of Attention in our Model and 4 Why Self-Attention).\\\\n\\\\n2) Key points drawn from the retrieved </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">context:\\\\n- Self-attention relates different positions of a single sequence to compute a representation of the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequence [3.2.3 Applications of Attention in our Model].\\\\n- It reduces the path length between long-range </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">dependencies, facilitating the learning of these dependencies [2 Background].\\\\n- Self-attention allows for </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">parallel computation with a constant number of sequential operations, unlike recurrent layers which require O(n) </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequential operations [4 Why Self-Attention].\\\\n- It is faster than recurrent layers when the sequence length is </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">smaller than the representation dimensionality [4 Why Self-Attention].\\\\n- Self-attention can yield more </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">interpretable models, as individual attention heads can learn to perform different tasks related to the syntactic </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">and semantic structure of the sentences [4 Why Self-Attention].\\\\n\\\\n3) Notes on mechanism details mentioned in the</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">retrieved context:\\\\n- The Transformer uses multi-head attention in encoder-decoder attention, encoder </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">self-attention, and decoder self-attention, with masking in the decoder to prevent leftward information flow and </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">preserve auto-regressive properties (scaled dot-product attention with masking) [3.2.3 Applications of Attention in</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">our Model].\\\\n- The context explicitly references scaled dot-product attention and masking as part of implementing </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">self-attention in the Transformer [3.2.3 Applications of Attention in our Model].\\\\n\\\\n4) Assumptions: None\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">  },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">  \"self attention computation in transformer models\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    \"retrieved_context\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"metadata\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"pk\": 463415957437808643,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"source\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"dl_meta\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"schema_name\": \"docling_core.transforms.chunker.DocMeta\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"version\": \"1.0.0\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"doc_items\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/13\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 107.691, \"t\": 490.609, \"r\": 505.241, \"b\": 394.784, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 825]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/14\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 108.0, \"t\": 386.947, \"r\": 505.25, \"b\": 345.668, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 393]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/15\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 108.0, \"t\": 337.831, \"r\": 505.654, \"b\": 307.461, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 217]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/16\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 107.691, \"t\": 299.625, \"r\": 505.657, \"b\": 258.346, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 373]}]}</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"headings\": [\"2 Background\"],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"origin\": {\"mimetype\": \"application/pdf\", \"binary_hash\": 1945408574365156663, \"filename\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"}</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page_content\": \"2 Background\\nThe goal of reducing sequential computation also forms the foundation of the</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">building block, computing hidden representations in parallel for all input and output positions. In these models, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">the number of operations required to relate signals from two arbitrary input or output positions grows in the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">distance between positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">with Multi-Head Attention as described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">attention mechanism relating different positions of a single sequence in order to compute a representation of the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">and have been shown to perform well on simple-language question answering and language modeling tasks [28].\\nTo the</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">to compute representations of its input and output without using sequencealigned RNNs or convolution. In the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">models such as [14, 15] and [8].\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"metadata\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"pk\": 463415957437808650,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"source\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          \"dl_meta\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"schema_name\": \"docling_core.transforms.chunker.DocMeta\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"version\": \"1.0.0\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"doc_items\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/100\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 107.691, \"t\": 549.551, \"r\": 372.606, \"b\": 540.999, \"coord_origin\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"BOTTOMLEFT\"}, \"charspan\": [0, 66]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/101\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 529.509, \"r\": 505.242, \"b\": 477.003, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 364]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/102\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 470.986, \"r\": 504.003, \"b\": 429.389, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 291]}]},</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">              {\"self_ref\": \"#/texts/103\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 423.372, \"r\": 504.003, \"b\": 370.865, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 446]}]}</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"headings\": [\"3.2.3 Applications of Attention in our Model\"],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">            \"origin\": {\"mimetype\": \"application/pdf\", \"binary_hash\": 1945408574365156663, \"filename\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"}</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">          }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page_content\": \"3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">three different ways:\\n- In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">and the memory keys and values come from the output of the encoder. This allows every position in the decoder to </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequence-to-sequence models such as [31, 2, 8].\\n- The encoder contains self-attention layers. In a self-attention </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">the decoder up to and including that position. We need to prevent leftward information flow in the decoder to </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">(setting to -âˆž ) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    \"answer_with_citations\": \"1) Final answer: In Transformer models, self-attention is used to compute </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">representations by relating different positions of a single sequence, allowing each position to attend to all </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">others with a constant number of operations, which is more efficient than recurrent layers that require O(n) </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">operations [Vaswani et al., 2017]. This mechanism is applied in both the encoder and decoder, with the decoder's </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">self-attention being masked to prevent leftward information flow and maintain the auto-regressive property (scaled </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">dot-product attention with masking) [Vaswani et al., 2017].\\\\n\\\\n2) Key points drawn from the retrieved </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">context:\\\\n- The encoder contains self-attention layers where keys, values, and queries come from the same source, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">allowing each position to attend to all positions in the previous encoder layer.\\\\n- The decoder contains </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">self-attention layers as well, but these are masked to prevent leftward information flow to preserve </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">autoregressiveness.\\\\n- In encoder-decoder attention, queries come from the decoder, while keys/values come from </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">the encoder output, enabling attention over the entire input sequence.\\\\n- The implementation uses scaled </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">dot-product attention with masking to enforce the autoregressive constraint in the decoder.\\\\n\\\\n3) Citations used </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">in the synthesized answer: [Vaswani et al., 2017] (Transformer paper)\\\\n\\\\n4) Equations/steps mentioned in the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">retrieved context include the concept of scaled dot-product attention and masking for the decoder, as described in </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">the source material.\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">  }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37m{\u001b[0m\n",
       "\u001b[2;37m  \"self attention mechanism definition purpose\": {\u001b[0m\n",
       "\u001b[2;37m    \"retrieved_context\": [\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"metadata\": {\u001b[0m\n",
       "\u001b[2;37m          \"pk\": 463415957437808650,\u001b[0m\n",
       "\u001b[2;37m          \"source\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\u001b[0m\n",
       "\u001b[2;37m          \"dl_meta\": {\u001b[0m\n",
       "\u001b[2;37m            \"schema_name\": \"docling_core.transforms.chunker.DocMeta\",\u001b[0m\n",
       "\u001b[2;37m            \"version\": \"1.0.0\",\u001b[0m\n",
       "\u001b[2;37m            \"doc_items\": [\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/100\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 107.691, \"t\": 549.551, \"r\": 372.606, \"b\": 540.999, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 66]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/101\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": \u001b[0m\n",
       "\u001b[2;37m\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 529.509, \"r\": 505.242, \"b\": 477.003, \u001b[0m\n",
       "\u001b[2;37m\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 364]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/102\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": \u001b[0m\n",
       "\u001b[2;37m\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 470.986, \"r\": 504.003, \"b\": 429.389, \u001b[0m\n",
       "\u001b[2;37m\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 291]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/103\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": \u001b[0m\n",
       "\u001b[2;37m\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 423.372, \"r\": 504.003, \"b\": 370.865, \u001b[0m\n",
       "\u001b[2;37m\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 446]}]}\u001b[0m\n",
       "\u001b[2;37m            ],\u001b[0m\n",
       "\u001b[2;37m            \"headings\": [\"3.2.3 Applications of Attention in our Model\"],\u001b[0m\n",
       "\u001b[2;37m            \"origin\": {\"mimetype\": \"application/pdf\", \"binary_hash\": 1945408574365156663, \"filename\": \u001b[0m\n",
       "\u001b[2;37m\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"}\u001b[0m\n",
       "\u001b[2;37m          }\u001b[0m\n",
       "\u001b[2;37m        },\u001b[0m\n",
       "\u001b[2;37m        \"page_content\": \"3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in\u001b[0m\n",
       "\u001b[2;37mthree different ways:\\n- In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,\u001b[0m\n",
       "\u001b[2;37mand the memory keys and values come from the output of the encoder. This allows every position in the decoder to \u001b[0m\n",
       "\u001b[2;37mattend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in \u001b[0m\n",
       "\u001b[2;37msequence-to-sequence models such as [31, 2, 8].\\n- The encoder contains self-attention layers. In a self-attention \u001b[0m\n",
       "\u001b[2;37mlayer all of the keys, values and queries come from the same place, in this case, the output of the previous layer \u001b[0m\n",
       "\u001b[2;37min the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- \u001b[0m\n",
       "\u001b[2;37mSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in \u001b[0m\n",
       "\u001b[2;37mthe decoder up to and including that position. We need to prevent leftward information flow in the decoder to \u001b[0m\n",
       "\u001b[2;37mpreserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out \u001b[0m\n",
       "\u001b[2;37m(setting to -âˆž ) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\"\u001b[0m\n",
       "\u001b[2;37m      },\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"metadata\": {\u001b[0m\n",
       "\u001b[2;37m          \"pk\": 463415957437808643,\u001b[0m\n",
       "\u001b[2;37m          \"source\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\u001b[0m\n",
       "\u001b[2;37m          \"dl_meta\": {\u001b[0m\n",
       "\u001b[2;37m            \"schema_name\": \"docling_core.transforms.chunker.DocMeta\",\u001b[0m\n",
       "\u001b[2;37m            \"version\": \"1.0.0\",\u001b[0m\n",
       "\u001b[2;37m            \"doc_items\": [\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/13\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 107.691, \"t\": 490.609, \"r\": 505.241, \"b\": 394.784, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 825]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/14\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 108.0, \"t\": 386.947, \"r\": 505.25, \"b\": 345.668, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 393]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/15\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 108.0, \"t\": 337.831, \"r\": 505.654, \"b\": 307.461, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 217]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/16\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 107.691, \"t\": 299.625, \"r\": 505.657, \"b\": 258.346, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 373]}]}\u001b[0m\n",
       "\u001b[2;37m            ],\u001b[0m\n",
       "\u001b[2;37m            \"headings\": [\"2 Background\"],\u001b[0m\n",
       "\u001b[2;37m            \"origin\": {\"mimetype\": \"application/pdf\", \"binary_hash\": 1945408574365156663, \"filename\": \u001b[0m\n",
       "\u001b[2;37m\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"}\u001b[0m\n",
       "\u001b[2;37m          }\u001b[0m\n",
       "\u001b[2;37m        },\u001b[0m\n",
       "\u001b[2;37m        \"page_content\": \"2 Background\\nThe goal of reducing sequential computation also forms the foundation of the\u001b[0m\n",
       "\u001b[2;37mExtended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic \u001b[0m\n",
       "\u001b[2;37mbuilding block, computing hidden representations in parallel for all input and output positions. In these models, \u001b[0m\n",
       "\u001b[2;37mthe number of operations required to relate signals from two arbitrary input or output positions grows in the \u001b[0m\n",
       "\u001b[2;37mdistance between positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at \u001b[0m\n",
       "\u001b[2;37mthe cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract \u001b[0m\n",
       "\u001b[2;37mwith Multi-Head Attention as described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an \u001b[0m\n",
       "\u001b[2;37mattention mechanism relating different positions of a single sequence in order to compute a representation of the \u001b[0m\n",
       "\u001b[2;37msequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, \u001b[0m\n",
       "\u001b[2;37mabstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, \u001b[0m\n",
       "\u001b[2;37m19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence\u001b[0m\n",
       "\u001b[2;37mand have been shown to perform well on simple-language question answering and language modeling tasks [28].\\nTo the\u001b[0m\n",
       "\u001b[2;37mbest of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention \u001b[0m\n",
       "\u001b[2;37mto compute representations of its input and output without using sequencealigned RNNs or convolution. In the \u001b[0m\n",
       "\u001b[2;37mfollowing sections, we will describe the Transformer, motivate self-attention and discuss its advantages over \u001b[0m\n",
       "\u001b[2;37mmodels such as [14, 15] and [8].\"\u001b[0m\n",
       "\u001b[2;37m      },\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"metadata\": {\u001b[0m\n",
       "\u001b[2;37m          \"pk\": 463415957437808654,\u001b[0m\n",
       "\u001b[2;37m          \"source\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\u001b[0m\n",
       "\u001b[2;37m          \"dl_meta\": {\u001b[0m\n",
       "\u001b[2;37m            \"schema_name\": \"docling_core.transforms.chunker.DocMeta\",\u001b[0m\n",
       "\u001b[2;37m            \"version\": \"1.0.0\",\u001b[0m\n",
       "\u001b[2;37m            \"doc_items\": [\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/120\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 6, \"bbox\": {\"l\": 106.834, \"t\": 313.106, \"r\": 505.654, \"b\": 260.918, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 434]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/121\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 6, \"bbox\": {\"l\": 108.0, \"t\": 253.08100000000002, \"r\": 504.0, \"b\": 233.62, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 184]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/122\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 6, \"bbox\": {\"l\": 107.691, \"t\": 225.784, \"r\": 504.001, \"b\": 151.77700000000004, \"coord_origin\":\u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 610]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/123\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 6, \"bbox\": {\"l\": 107.641, \"t\": 143.94100000000003, \"r\": 504.345, \"b\": 69.93399999999997, \u001b[0m\n",
       "\u001b[2;37m\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 703]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/125\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 7, \"bbox\": {\"l\": 108.0, \"t\": 716.523, \"r\": 504.001, \"b\": 697.062, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 186]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/126\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 7, \"bbox\": {\"l\": 107.641, \"t\": 689.544, \"r\": 505.248, \"b\": 604.31, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 745]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/127\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 7, \"bbox\": {\"l\": 107.641, \"t\": 596.474, \"r\": 504.004, \"b\": 555.194, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 350]}]}\u001b[0m\n",
       "\u001b[2;37m            ],\u001b[0m\n",
       "\u001b[2;37m            \"headings\": [\"4 Why Self-Attention\"],\u001b[0m\n",
       "\u001b[2;37m            \"origin\": {\"mimetype\": \"application/pdf\", \"binary_hash\": 1945408574365156663, \"filename\": \u001b[0m\n",
       "\u001b[2;37m\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"}\u001b[0m\n",
       "\u001b[2;37m          }\u001b[0m\n",
       "\u001b[2;37m        },\u001b[0m\n",
       "\u001b[2;37m        \"page_content\": \"4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers \u001b[0m\n",
       "\u001b[2;37mto the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol \u001b[0m\n",
       "\u001b[2;37mrepresentations ( x 1 , ..., x n ) to another sequence of equal length ( z 1 , ..., z n ) , with x i , z i âˆˆ R d , \u001b[0m\n",
       "\u001b[2;37msuch as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention \u001b[0m\n",
       "\u001b[2;37mwe consider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of \u001b[0m\n",
       "\u001b[2;37mcomputation that can be parallelized, as measured by the minimum number of sequential operations required.\\nThe \u001b[0m\n",
       "\u001b[2;37mthird is the path length between long-range dependencies in the network. Learning long-range dependencies is a key \u001b[0m\n",
       "\u001b[2;37mchallenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is \u001b[0m\n",
       "\u001b[2;37mthe length of the paths forward and backward signals have to traverse in the network. The shorter these paths \u001b[0m\n",
       "\u001b[2;37mbetween any combination of positions in the input and output sequences, the easier it is to learn long-range \u001b[0m\n",
       "\u001b[2;37mdependencies [11]. Hence we also compare the maximum path length between any two input and output positions in \u001b[0m\n",
       "\u001b[2;37mnetworks composed of the different layer types.\\nAs noted in Table 1, a self-attention layer connects all positions\u001b[0m\n",
       "\u001b[2;37mwith a constant number of sequentially executed operations, whereas a recurrent layer requires O ( n ) sequential \u001b[0m\n",
       "\u001b[2;37moperations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the \u001b[0m\n",
       "\u001b[2;37msequence length n is smaller than the representation dimensionality d , which is most often the case with sentence \u001b[0m\n",
       "\u001b[2;37mrepresentations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25]\u001b[0m\n",
       "\u001b[2;37mrepresentations. To improve computational performance for tasks involving very long sequences, self-attention could\u001b[0m\n",
       "\u001b[2;37mbe restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective \u001b[0m\n",
       "\u001b[2;37moutput position. This would increase the maximum path length to O ( n/r ) . We plan to investigate this approach \u001b[0m\n",
       "\u001b[2;37mfurther in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input \u001b[0m\n",
       "\u001b[2;37mand output positions. Doing so requires a stack of O ( n/k ) convolutional layers in the case of contiguous \u001b[0m\n",
       "\u001b[2;37mkernels, or O ( log k ( n )) in the case of dilated convolutions [15], increasing the length of the longest paths \u001b[0m\n",
       "\u001b[2;37mbetween any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, \u001b[0m\n",
       "\u001b[2;37mby a factor of k . Separable convolutions [6], however, decrease the complexity considerably, to O ( k Â· n Â· d + n \u001b[0m\n",
       "\u001b[2;37mÂ· d 2 ) . Even with k = n , however, the complexity of a separable convolution is equal to the combination of a \u001b[0m\n",
       "\u001b[2;37mself-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\nAs side benefit, \u001b[0m\n",
       "\u001b[2;37mself-attention could yield more interpretable models. We inspect attention distributions from our models and \u001b[0m\n",
       "\u001b[2;37mpresent and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform \u001b[0m\n",
       "\u001b[2;37mdifferent tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\"\u001b[0m\n",
       "\u001b[2;37m      }\u001b[0m\n",
       "\u001b[2;37m    ],\u001b[0m\n",
       "\u001b[2;37m    \"answer_with_citations\": \"1) Final answer: The self-attention mechanism, also known as intra-attention, relates\u001b[0m\n",
       "\u001b[2;37mdifferent positions of a single sequence to compute a representation of the sequence. It is used to reduce the path\u001b[0m\n",
       "\u001b[2;37mlength between long-range dependencies, making it easier to learn such dependencies, and allows for parallel \u001b[0m\n",
       "\u001b[2;37mcomputation with a constant number of sequential operations [Vaswani et al., 2017] (as discussed in 3.2.3 \u001b[0m\n",
       "\u001b[2;37mApplications of Attention in our Model and 4 Why Self-Attention).\\\\n\\\\n2) Key points drawn from the retrieved \u001b[0m\n",
       "\u001b[2;37mcontext:\\\\n- Self-attention relates different positions of a single sequence to compute a representation of the \u001b[0m\n",
       "\u001b[2;37msequence [3.2.3 Applications of Attention in our Model].\\\\n- It reduces the path length between long-range \u001b[0m\n",
       "\u001b[2;37mdependencies, facilitating the learning of these dependencies [2 Background].\\\\n- Self-attention allows for \u001b[0m\n",
       "\u001b[2;37mparallel computation with a constant number of sequential operations, unlike recurrent layers which require O(n) \u001b[0m\n",
       "\u001b[2;37msequential operations [4 Why Self-Attention].\\\\n- It is faster than recurrent layers when the sequence length is \u001b[0m\n",
       "\u001b[2;37msmaller than the representation dimensionality [4 Why Self-Attention].\\\\n- Self-attention can yield more \u001b[0m\n",
       "\u001b[2;37minterpretable models, as individual attention heads can learn to perform different tasks related to the syntactic \u001b[0m\n",
       "\u001b[2;37mand semantic structure of the sentences [4 Why Self-Attention].\\\\n\\\\n3) Notes on mechanism details mentioned in the\u001b[0m\n",
       "\u001b[2;37mretrieved context:\\\\n- The Transformer uses multi-head attention in encoder-decoder attention, encoder \u001b[0m\n",
       "\u001b[2;37mself-attention, and decoder self-attention, with masking in the decoder to prevent leftward information flow and \u001b[0m\n",
       "\u001b[2;37mpreserve auto-regressive properties (scaled dot-product attention with masking) [3.2.3 Applications of Attention in\u001b[0m\n",
       "\u001b[2;37mour Model].\\\\n- The context explicitly references scaled dot-product attention and masking as part of implementing \u001b[0m\n",
       "\u001b[2;37mself-attention in the Transformer [3.2.3 Applications of Attention in our Model].\\\\n\\\\n4) Assumptions: None\"\u001b[0m\n",
       "\u001b[2;37m  },\u001b[0m\n",
       "\u001b[2;37m  \"self attention computation in transformer models\": {\u001b[0m\n",
       "\u001b[2;37m    \"retrieved_context\": [\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"metadata\": {\u001b[0m\n",
       "\u001b[2;37m          \"pk\": 463415957437808643,\u001b[0m\n",
       "\u001b[2;37m          \"source\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\u001b[0m\n",
       "\u001b[2;37m          \"dl_meta\": {\u001b[0m\n",
       "\u001b[2;37m            \"schema_name\": \"docling_core.transforms.chunker.DocMeta\",\u001b[0m\n",
       "\u001b[2;37m            \"version\": \"1.0.0\",\u001b[0m\n",
       "\u001b[2;37m            \"doc_items\": [\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/13\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 107.691, \"t\": 490.609, \"r\": 505.241, \"b\": 394.784, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 825]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/14\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 108.0, \"t\": 386.947, \"r\": 505.25, \"b\": 345.668, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 393]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/15\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 108.0, \"t\": 337.831, \"r\": 505.654, \"b\": 307.461, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 217]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/16\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 2, \"bbox\": {\"l\": 107.691, \"t\": 299.625, \"r\": 505.657, \"b\": 258.346, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 373]}]}\u001b[0m\n",
       "\u001b[2;37m            ],\u001b[0m\n",
       "\u001b[2;37m            \"headings\": [\"2 Background\"],\u001b[0m\n",
       "\u001b[2;37m            \"origin\": {\"mimetype\": \"application/pdf\", \"binary_hash\": 1945408574365156663, \"filename\": \u001b[0m\n",
       "\u001b[2;37m\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"}\u001b[0m\n",
       "\u001b[2;37m          }\u001b[0m\n",
       "\u001b[2;37m        },\u001b[0m\n",
       "\u001b[2;37m        \"page_content\": \"2 Background\\nThe goal of reducing sequential computation also forms the foundation of the\u001b[0m\n",
       "\u001b[2;37mExtended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic \u001b[0m\n",
       "\u001b[2;37mbuilding block, computing hidden representations in parallel for all input and output positions. In these models, \u001b[0m\n",
       "\u001b[2;37mthe number of operations required to relate signals from two arbitrary input or output positions grows in the \u001b[0m\n",
       "\u001b[2;37mdistance between positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at \u001b[0m\n",
       "\u001b[2;37mthe cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract \u001b[0m\n",
       "\u001b[2;37mwith Multi-Head Attention as described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an \u001b[0m\n",
       "\u001b[2;37mattention mechanism relating different positions of a single sequence in order to compute a representation of the \u001b[0m\n",
       "\u001b[2;37msequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, \u001b[0m\n",
       "\u001b[2;37mabstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, \u001b[0m\n",
       "\u001b[2;37m19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence\u001b[0m\n",
       "\u001b[2;37mand have been shown to perform well on simple-language question answering and language modeling tasks [28].\\nTo the\u001b[0m\n",
       "\u001b[2;37mbest of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention \u001b[0m\n",
       "\u001b[2;37mto compute representations of its input and output without using sequencealigned RNNs or convolution. In the \u001b[0m\n",
       "\u001b[2;37mfollowing sections, we will describe the Transformer, motivate self-attention and discuss its advantages over \u001b[0m\n",
       "\u001b[2;37mmodels such as [14, 15] and [8].\"\u001b[0m\n",
       "\u001b[2;37m      },\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"metadata\": {\u001b[0m\n",
       "\u001b[2;37m          \"pk\": 463415957437808650,\u001b[0m\n",
       "\u001b[2;37m          \"source\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\u001b[0m\n",
       "\u001b[2;37m          \"dl_meta\": {\u001b[0m\n",
       "\u001b[2;37m            \"schema_name\": \"docling_core.transforms.chunker.DocMeta\",\u001b[0m\n",
       "\u001b[2;37m            \"version\": \"1.0.0\",\u001b[0m\n",
       "\u001b[2;37m            \"doc_items\": [\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/100\", \"parent\": {\"$ref\": \"#/body\"}, \"content_layer\": \"body\", \"label\": \"text\", \u001b[0m\n",
       "\u001b[2;37m\"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 107.691, \"t\": 549.551, \"r\": 372.606, \"b\": 540.999, \"coord_origin\": \u001b[0m\n",
       "\u001b[2;37m\"BOTTOMLEFT\"}, \"charspan\": [0, 66]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/101\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": \u001b[0m\n",
       "\u001b[2;37m\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 529.509, \"r\": 505.242, \"b\": 477.003, \u001b[0m\n",
       "\u001b[2;37m\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 364]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/102\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": \u001b[0m\n",
       "\u001b[2;37m\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 470.986, \"r\": 504.003, \"b\": 429.389, \u001b[0m\n",
       "\u001b[2;37m\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 291]}]},\u001b[0m\n",
       "\u001b[2;37m              {\"self_ref\": \"#/texts/103\", \"parent\": {\"$ref\": \"#/groups/0\"}, \"content_layer\": \"body\", \"label\": \u001b[0m\n",
       "\u001b[2;37m\"list_item\", \"prov\": [{\"page_no\": 5, \"bbox\": {\"l\": 133.903, \"t\": 423.372, \"r\": 504.003, \"b\": 370.865, \u001b[0m\n",
       "\u001b[2;37m\"coord_origin\": \"BOTTOMLEFT\"}, \"charspan\": [0, 446]}]}\u001b[0m\n",
       "\u001b[2;37m            ],\u001b[0m\n",
       "\u001b[2;37m            \"headings\": [\"3.2.3 Applications of Attention in our Model\"],\u001b[0m\n",
       "\u001b[2;37m            \"origin\": {\"mimetype\": \"application/pdf\", \"binary_hash\": 1945408574365156663, \"filename\": \u001b[0m\n",
       "\u001b[2;37m\"NIPS-2017-attention-is-all-you-need-Paper.pdf\"}\u001b[0m\n",
       "\u001b[2;37m          }\u001b[0m\n",
       "\u001b[2;37m        },\u001b[0m\n",
       "\u001b[2;37m        \"page_content\": \"3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in\u001b[0m\n",
       "\u001b[2;37mthree different ways:\\n- In \\\"encoder-decoder attention\\\" layers, the queries come from the previous decoder layer,\u001b[0m\n",
       "\u001b[2;37mand the memory keys and values come from the output of the encoder. This allows every position in the decoder to \u001b[0m\n",
       "\u001b[2;37mattend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in \u001b[0m\n",
       "\u001b[2;37msequence-to-sequence models such as [31, 2, 8].\\n- The encoder contains self-attention layers. In a self-attention \u001b[0m\n",
       "\u001b[2;37mlayer all of the keys, values and queries come from the same place, in this case, the output of the previous layer \u001b[0m\n",
       "\u001b[2;37min the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n- \u001b[0m\n",
       "\u001b[2;37mSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in \u001b[0m\n",
       "\u001b[2;37mthe decoder up to and including that position. We need to prevent leftward information flow in the decoder to \u001b[0m\n",
       "\u001b[2;37mpreserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out \u001b[0m\n",
       "\u001b[2;37m(setting to -âˆž ) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\"\u001b[0m\n",
       "\u001b[2;37m      }\u001b[0m\n",
       "\u001b[2;37m    ],\u001b[0m\n",
       "\u001b[2;37m    \"answer_with_citations\": \"1) Final answer: In Transformer models, self-attention is used to compute \u001b[0m\n",
       "\u001b[2;37mrepresentations by relating different positions of a single sequence, allowing each position to attend to all \u001b[0m\n",
       "\u001b[2;37mothers with a constant number of operations, which is more efficient than recurrent layers that require O(n) \u001b[0m\n",
       "\u001b[2;37moperations [Vaswani et al., 2017]. This mechanism is applied in both the encoder and decoder, with the decoder's \u001b[0m\n",
       "\u001b[2;37mself-attention being masked to prevent leftward information flow and maintain the auto-regressive property (scaled \u001b[0m\n",
       "\u001b[2;37mdot-product attention with masking) [Vaswani et al., 2017].\\\\n\\\\n2) Key points drawn from the retrieved \u001b[0m\n",
       "\u001b[2;37mcontext:\\\\n- The encoder contains self-attention layers where keys, values, and queries come from the same source, \u001b[0m\n",
       "\u001b[2;37mallowing each position to attend to all positions in the previous encoder layer.\\\\n- The decoder contains \u001b[0m\n",
       "\u001b[2;37mself-attention layers as well, but these are masked to prevent leftward information flow to preserve \u001b[0m\n",
       "\u001b[2;37mautoregressiveness.\\\\n- In encoder-decoder attention, queries come from the decoder, while keys/values come from \u001b[0m\n",
       "\u001b[2;37mthe encoder output, enabling attention over the entire input sequence.\\\\n- The implementation uses scaled \u001b[0m\n",
       "\u001b[2;37mdot-product attention with masking to enforce the autoregressive constraint in the decoder.\\\\n\\\\n3) Citations used \u001b[0m\n",
       "\u001b[2;37min the synthesized answer: [Vaswani et al., 2017] (Transformer paper)\\\\n\\\\n4) Equations/steps mentioned in the \u001b[0m\n",
       "\u001b[2;37mretrieved context include the concept of scaled dot-product attention and masking for the decoder, as described in \u001b[0m\n",
       "\u001b[2;37mthe source material.\"\u001b[0m\n",
       "\u001b[2;37m  }\u001b[0m\n",
       "\u001b[2;37m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 10: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 10: \u001b[0m\u001b[1;36mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 11: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Updated todo list to [{'content': \"DECOMPOSE: Anal...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 11: \u001b[0m\u001b[1;36mUpdated todo list to [{'content': \"DECOMPOSE: Anal...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Updated todo list to [{'content': \"DECOMPOSE: Analyze user's query and generate refined sub-queries for vector </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">retrieval.\", 'status': 'completed'}, {'content': 'VALIDATE: Ensure sub-queries are non-overlapping and atomic.', </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">'status': 'completed'}, {'content': 'RETRIEVE: Pass all sub-queries to vectorstore_retrieval_subagent in a single </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">batch and fetch contexts with citations.', 'status': 'completed'}, {'content': 'AGGREGATE: Organize retrieved </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">results by sub-query, preserving citation metadata.', 'status': 'in_progress'}, {'content': 'SYNTHESIZE: Combine </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">answers into a concise, cited final response with key points and assumptions.', 'status': 'pending'}, {'content': </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">'REVIEW: Verify all claims are backed by retrieved citations and finalize response.', 'status': 'pending'}]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mUpdated todo list to [{'content': \"DECOMPOSE: Analyze user's query and generate refined sub-queries for vector \u001b[0m\n",
       "\u001b[2;37mretrieval.\", 'status': 'completed'}, {'content': 'VALIDATE: Ensure sub-queries are non-overlapping and atomic.', \u001b[0m\n",
       "\u001b[2;37m'status': 'completed'}, {'content': 'RETRIEVE: Pass all sub-queries to vectorstore_retrieval_subagent in a single \u001b[0m\n",
       "\u001b[2;37mbatch and fetch contexts with citations.', 'status': 'completed'}, {'content': 'AGGREGATE: Organize retrieved \u001b[0m\n",
       "\u001b[2;37mresults by sub-query, preserving citation metadata.', 'status': 'in_progress'}, {'content': 'SYNTHESIZE: Combine \u001b[0m\n",
       "\u001b[2;37manswers into a concise, cited final response with key points and assumptions.', 'status': 'pending'}, {'content': \u001b[0m\n",
       "\u001b[2;37m'REVIEW: Verify all claims are backed by retrieved citations and finalize response.', 'status': 'pending'}]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 12: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 12: \u001b[0m\u001b[1;36mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 13: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Updated todo list to [{'content': \"DECOMPOSE: Anal...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 13: \u001b[0m\u001b[1;36mUpdated todo list to [{'content': \"DECOMPOSE: Anal...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Updated todo list to [{'content': \"DECOMPOSE: Analyze user's query and generate refined sub-queries for vector </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">retrieval.\", 'status': 'completed'}, {'content': 'VALIDATE: Ensure sub-queries are non-overlapping and atomic.', </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">'status': 'completed'}, {'content': 'RETRIEVE: Pass all sub-queries to vectorstore_retrieval_subagent in a single </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">batch and fetch contexts with citations.', 'status': 'completed'}, {'content': 'AGGREGATE: Organize retrieved </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">results by sub-query, preserving citation metadata.', 'status': 'completed'}, {'content': 'SYNTHESIZE: Combine </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">answers into a concise, cited final response with key points and assumptions.', 'status': 'completed'}, {'content':</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">'REVIEW: Verify all claims are backed by retrieved citations and finalize response.', 'status': 'completed'}]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mUpdated todo list to [{'content': \"DECOMPOSE: Analyze user's query and generate refined sub-queries for vector \u001b[0m\n",
       "\u001b[2;37mretrieval.\", 'status': 'completed'}, {'content': 'VALIDATE: Ensure sub-queries are non-overlapping and atomic.', \u001b[0m\n",
       "\u001b[2;37m'status': 'completed'}, {'content': 'RETRIEVE: Pass all sub-queries to vectorstore_retrieval_subagent in a single \u001b[0m\n",
       "\u001b[2;37mbatch and fetch contexts with citations.', 'status': 'completed'}, {'content': 'AGGREGATE: Organize retrieved \u001b[0m\n",
       "\u001b[2;37mresults by sub-query, preserving citation metadata.', 'status': 'completed'}, {'content': 'SYNTHESIZE: Combine \u001b[0m\n",
       "\u001b[2;37manswers into a concise, cited final response with key points and assumptions.', 'status': 'completed'}, {'content':\u001b[0m\n",
       "\u001b[2;37m'REVIEW: Verify all claims are backed by retrieved citations and finalize response.', 'status': 'completed'}]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 14: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">{\"results\":[{\"sub_query\":\"self attention mechanism...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 14: \u001b[0m\u001b[1;36m{\"results\":[{\"sub_query\":\"self attention mechanism...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">{\"results\":[{\"sub_query\":\"self attention mechanism definition purpose\",\"retrieved_context\":\"Vaswani et al., </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\\\"Attention Is All You Need\\\" (Transformer paper) â€” sections 2 Background, 3.2.3 Applications of Attention in our </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Model, and 4 Why Self-Attention describe self-attention (intra-attention) as an attention mechanism that relates </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">different positions of a single sequence to compute a sequence representation; they motivate it by reduced path </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">length for long-range dependencies, parallelizability, and interpretability versus </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">RNNs/convolutions.\",\"citations\":[\"Vaswani et al., 2017 (NIPS paper)\"],\"synthesized_answer\":\"Self-attention </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">(intra-attention) relates different positions within one sequence to compute contextualized representations, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">reducing the path length for long-range dependencies and enabling parallel computation across positions [Vaswani et</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">al., 2017].\"},{\"sub_query\":\"self attention computation in transformer models\",\"retrieved_context\":\"Vaswani et al., </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\\\"Attention Is All You Need\\\" â€” sections 2 and 3.2.3 explain that in the Transformer, keys, queries and values are </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">derived from the same layer for encoder self-attention; attention is implemented as scaled dot-product attention; </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">multi-head attention runs parallel attention heads to capture multiple relations; decoder self-attention is masked </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">to prevent leftward information flow and preserve autoregressiveness [Vaswani et al., 2017].\",\"citations\":[\"Vaswani</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">et al., 2017 (NIPS paper)\"],\"synthesized_answer\":\"In Transformers, self-attention computes attention weights using </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">scaled dot-product attention between queries, keys, and values from the same source, is executed in parallel via </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">multiple heads, and uses masking in the decoder to enforce autoregressive generation [Vaswani et al., 2017].\"}]}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37m{\"results\":[{\"sub_query\":\"self attention mechanism definition purpose\",\"retrieved_context\":\"Vaswani et al., \u001b[0m\n",
       "\u001b[2;37m\\\"Attention Is All You Need\\\" (Transformer paper) â€” sections 2 Background, 3.2.3 Applications of Attention in our \u001b[0m\n",
       "\u001b[2;37mModel, and 4 Why Self-Attention describe self-attention (intra-attention) as an attention mechanism that relates \u001b[0m\n",
       "\u001b[2;37mdifferent positions of a single sequence to compute a sequence representation; they motivate it by reduced path \u001b[0m\n",
       "\u001b[2;37mlength for long-range dependencies, parallelizability, and interpretability versus \u001b[0m\n",
       "\u001b[2;37mRNNs/convolutions.\",\"citations\":[\"Vaswani et al., 2017 (NIPS paper)\"],\"synthesized_answer\":\"Self-attention \u001b[0m\n",
       "\u001b[2;37m(intra-attention) relates different positions within one sequence to compute contextualized representations, \u001b[0m\n",
       "\u001b[2;37mreducing the path length for long-range dependencies and enabling parallel computation across positions [Vaswani et\u001b[0m\n",
       "\u001b[2;37mal., 2017].\"},{\"sub_query\":\"self attention computation in transformer models\",\"retrieved_context\":\"Vaswani et al., \u001b[0m\n",
       "\u001b[2;37m\\\"Attention Is All You Need\\\" â€” sections 2 and 3.2.3 explain that in the Transformer, keys, queries and values are \u001b[0m\n",
       "\u001b[2;37mderived from the same layer for encoder self-attention; attention is implemented as scaled dot-product attention; \u001b[0m\n",
       "\u001b[2;37mmulti-head attention runs parallel attention heads to capture multiple relations; decoder self-attention is masked \u001b[0m\n",
       "\u001b[2;37mto prevent leftward information flow and preserve autoregressiveness [Vaswani et al., 2017].\",\"citations\":[\"Vaswani\u001b[0m\n",
       "\u001b[2;37met al., 2017 (NIPS paper)\"],\"synthesized_answer\":\"In Transformers, self-attention computes attention weights using \u001b[0m\n",
       "\u001b[2;37mscaled dot-product attention between queries, keys, and values from the same source, is executed in parallel via \u001b[0m\n",
       "\u001b[2;37mmultiple heads, and uses masking in the decoder to enforce autoregressive generation [Vaswani et al., 2017].\"}]}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚ ðŸ“Š Retrieved Context &amp; Answers â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[1;32mâ”‚\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32mðŸ“Š Retrieved Context & Answers\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32mâ”‚\u001b[0m\n",
       "\u001b[1;32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">================================================================================\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "================================================================================\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sub-Query 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">self attention mechanism definition purpose</span> <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m Sub-Query 1 \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m \u001b[1;37mself attention mechanism definition purpose\u001b[0m \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Answer:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mAnswer:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Self-attention (intra-attention) relates different positions within one sequence to compute contextualized </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representations, reducing the path length for long-range dependencies and enabling parallel computation across </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">positions [Vaswani et al., 2017].</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mSelf-attention (intra-attention) relates different positions within one sequence to compute contextualized \u001b[0m\n",
       "\u001b[32mrepresentations, reducing the path length for long-range dependencies and enabling parallel computation across \u001b[0m\n",
       "\u001b[32mpositions [Vaswani et al., 2017].\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Citations:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35mCitations:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  â€¢ Vaswani et al., 2017 (NIPS paper)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m  â€¢ Vaswani et al., 2017 (NIPS paper)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Context Preview:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33mContext Preview:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Vaswani et al., \"Attention Is All You Need\" (Transformer paper) â€” sections 2 Background, 3.2.3 Applications of </span>\n",
       "<span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Attention in our Model, and 4 Why Self-Attention describe self-attention (intra-attention...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;33mVaswani et al., \"Attention Is All You Need\" (Transformer paper) â€” sections 2 Background, 3.2.3 Applications of \u001b[0m\n",
       "\u001b[2;33mAttention in our Model, and 4 Why Self-Attention describe self-attention (intra-attention...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sub-Query 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">self attention computation in transformer models</span> <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m Sub-Query 2 \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m \u001b[1;37mself attention computation in transformer models\u001b[0m \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Answer:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mAnswer:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">In Transformers, self-attention computes attention weights using scaled dot-product attention between queries, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">keys, and values from the same source, is executed in parallel via multiple heads, and uses masking in the decoder </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to enforce autoregressive generation [Vaswani et al., 2017].</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mIn Transformers, self-attention computes attention weights using scaled dot-product attention between queries, \u001b[0m\n",
       "\u001b[32mkeys, and values from the same source, is executed in parallel via multiple heads, and uses masking in the decoder \u001b[0m\n",
       "\u001b[32mto enforce autoregressive generation [Vaswani et al., 2017].\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Citations:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35mCitations:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  â€¢ Vaswani et al., 2017 (NIPS paper)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m  â€¢ Vaswani et al., 2017 (NIPS paper)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Context Preview:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33mContext Preview:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Vaswani et al., \"Attention Is All You Need\" â€” sections 2 and 3.2.3 explain that in the Transformer, keys, queries </span>\n",
       "<span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">and values are derived from the same layer for encoder self-attention; attention is im...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;33mVaswani et al., \"Attention Is All You Need\" â€” sections 2 and 3.2.3 explain that in the Transformer, keys, queries \u001b[0m\n",
       "\u001b[2;33mand values are derived from the same layer for encoder self-attention; attention is im...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich.table import Table\n",
    "\n",
    "console = Console()\n",
    "\n",
    "console.print(\"\\n\" + \"=\"*80)\n",
    "console.print(Panel.fit(\"ðŸ¤– Agent Execution Summary\", style=\"bold cyan\"))\n",
    "console.print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for i, msg in enumerate(result['messages'], 1):\n",
    "    msg_content = msg.content if hasattr(msg, 'content') else str(msg)\n",
    "    if len(msg_content.strip()) == 0:\n",
    "        msg_content = \"Tool Calls: \"\n",
    "        msg_content += [tool['name'] for tool in msg.tool_calls][0]\n",
    "    header = Text(f\"Step {i}: \", style=\"bold yellow\")\n",
    "    header.append(msg_content[:50] + \"...\" if len(msg_content) > 50 else msg_content, style=\"cyan\")\n",
    "    console.print(header)\n",
    "    console.print(Text(msg_content, style=\"white dim\"))\n",
    "    console.print()\n",
    "\n",
    "# Display structured results\n",
    "console.print(\"\\n\" + \"=\"*80)\n",
    "console.print(Panel.fit(\"ðŸ“Š Retrieved Context & Answers\", style=\"bold green\"))\n",
    "console.print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if \"structured_response\" in result and \"results\" in result[\"structured_response\"]:\n",
    "    for idx, item in enumerate(result[\"structured_response\"][\"results\"], 1):\n",
    "        # Sub-query panel\n",
    "        console.print(Panel(\n",
    "            Text(item.get(\"sub_query\", \"N/A\"), style=\"bold white\"),\n",
    "            title=f\"Sub-Query {idx}\",\n",
    "            border_style=\"cyan\",\n",
    "            expand=False\n",
    "        ))\n",
    "        \n",
    "        # Synthesized answer\n",
    "        if item.get(\"synthesized_answer\"):\n",
    "            console.print(Text(\"Answer:\", style=\"bold green\"))\n",
    "            console.print(Text(item[\"synthesized_answer\"], style=\"green\"))\n",
    "        \n",
    "        # Citations\n",
    "        if item.get(\"citations\"):\n",
    "            console.print(Text(\"\\nCitations:\", style=\"bold magenta\"))\n",
    "            for citation in item[\"citations\"]:\n",
    "                console.print(Text(f\"  â€¢ {citation}\", style=\"magenta\"))\n",
    "        \n",
    "        # Context preview\n",
    "        if item.get(\"retrieved_context\"):\n",
    "            context_preview = item[\"retrieved_context\"][:200] + \"...\" if len(item[\"retrieved_context\"]) > 200 else item[\"retrieved_context\"]\n",
    "            console.print(Text(f\"\\nContext Preview:\", style=\"bold yellow\"))\n",
    "            console.print(Text(context_preview, style=\"yellow dim\"))\n",
    "        \n",
    "        console.print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6a0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
