{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a348d68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28c6d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sultan/Projects/citebase/agents/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from deepagents import create_deep_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc9e87",
   "metadata": {},
   "source": [
    "**RAG.**\n",
    "Define a retrieval function that the deep_agent will use as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf2f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\n",
    "TOP_K = 3\n",
    "queries = ['mechanics of scaled dot product attention',\n",
    " 'key aspects of multi head attention']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54dd9bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 10:57:56,752 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-07 10:57:56,845 - INFO - Going to convert document batch...\n",
      "2026-01-07 10:57:56,845 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2026-01-07 10:57:56,860 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-07 10:57:56,860 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-07 10:57:56,863 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-07 10:57:56,869 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-07 10:57:56,870 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-07 10:57:56,873 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-07 10:57:57,718 - INFO - Auto OCR model selected ocrmac.\n",
      "2026-01-07 10:57:57,724 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-07 10:57:57,724 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-07 10:57:57,728 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-07 10:57:57,733 - INFO - Accelerator device: 'mps'\n",
      "2026-01-07 10:57:59,771 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-07 10:57:59,774 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-07 10:57:59,777 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-07 10:58:00,655 - INFO - Accelerator device: 'mps'\n",
      "2026-01-07 10:58:01,418 - INFO - Processing document NIPS-2017-attention-is-all-you-need-Paper.pdf\n",
      "2026-01-07 10:58:13,109 - INFO - Finished converting document NIPS-2017-attention-is-all-you-need-Paper.pdf in 19.18 sec.\n",
      "/Users/sultan/Projects/citebase/agents/.venv/lib/python3.14/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2026-01-07 10:58:15,858 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "import tiktoken\n",
    "from docling_core.transforms.chunker.tokenizer.openai import OpenAITokenizer\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenizer = OpenAITokenizer(\n",
    "    tokenizer=enc,\n",
    "    max_tokens=128 * 1024,  # set to the model's context window\n",
    ")\n",
    "\n",
    "loader = DoclingLoader(file_path=FILE_PATH, chunker=HybridChunker(tokenizer=tokenizer))\n",
    "docs = loader.load()\n",
    "\n",
    "milvus_uri = str(Path(mkdtemp()) / \"vector.db\")\n",
    "\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"vectordb\",\n",
    "    connection_args={\"uri\": milvus_uri},\n",
    "    index_params={\"index_type\": \"FLAT\"},\n",
    "    drop_old=True,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "\"\"\"You must answer using ONLY the context below. Do not use outside knowledge.\n",
    "\n",
    "CONTEXT (each excerpt includes its citation tag like [source:... page:... chunk:...])\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "\n",
    "QUERY: {input}\n",
    "\n",
    "CITATION RULES\n",
    "- Every factual claim must end with a citation tag copied from the context, like: [source:XYZ page:12 chunk:5].\n",
    "- If a sentence contains multiple claims from different excerpts, include multiple citation tags at the end of that sentence.\n",
    "- Do NOT invent citation tags. Use only tags that appear in the context verbatim.\n",
    "\n",
    "Return exactly:\n",
    "1) Final answer (short, 2â€“6 sentences, with citations)\n",
    "2) Key points (3â€“7 bullets, each bullet with citations)\n",
    "3) Assumptions (or \"None\")\n",
    "If the context doesn't support the answer, say: \"Not answerable from context.\"\n",
    "\n",
    "RESPONSE:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9247c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def retrieve_from_vectorstore(queries: list[str]) -> dict:\n",
    "    \"\"\"Retrieve and generate answers from the vectorstore for a list of queries.\n",
    "    This function invokes the RAG chain for each query and returns aggregated results.\"\"\"\n",
    "    \n",
    "    resp_dict={}\n",
    "    for query in queries:\n",
    "        resp_dict[query] = rag_chain.invoke({\"input\": query})\n",
    "        \n",
    "    return resp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cf964",
   "metadata": {},
   "source": [
    "**Prepare settings for subagents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd667a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"\"\"\n",
    "Example 1 single information need\n",
    "Input query:\n",
    "How does dropout prevent overfitting in neural networks\n",
    "Output:\n",
    "{\"sub_queries\":[\"dropout regularization reducing neural network overfitting\"]}\n",
    "\n",
    "Example 2 two distinct information needs\n",
    "Input query:\n",
    "effects of microplastics on marine food webs and human health risks\n",
    "Output:\n",
    "{\"sub_queries\":[\"microplastics impacts on marine food webs\",\"human health risks from microplastic exposure\"]}\n",
    "\n",
    "Example 3 avoid over splitting broad impact\n",
    "Input query:\n",
    "What are the latest advancements in natural language processing and how do they impact machine learning models\n",
    "Output:\n",
    "{\"sub_queries\":[\"recent advancements in natural language processing\",\"impact of recent NLP advances on machine learning models\"]}\n",
    "\n",
    "Example 4 three distinct information needs\n",
    "Input query:\n",
    "role of gut microbiome in obesity and type 2 diabetes and dietary interventions\n",
    "Output:\n",
    "{\"sub_queries\":[\"gut microbiome links to obesity\",\"gut microbiome links to type 2 diabetes\",\"dietary interventions modulating gut microbiome in metabolic disease\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "query_decomposition_prompt = \"\"\"\n",
    "You are an academic query decomposition and refinement agent for vector retrieval.\n",
    "You will receive a query from a user which may contain single or multiple distinct information needs.\n",
    "\n",
    "Here are the examples which are done well:\n",
    "{examples}\n",
    "\n",
    "Goal:\n",
    "Return a list of refined sub-queries for retrieving relevant academic context from a vector database.\n",
    "\n",
    "Output (strict):\n",
    "- Return ONLY an object with key \"sub_queries\" containing a list of strings.\n",
    "- Each string must be a refined retrieval query.\n",
    "- Do NOT add any other keys or any extra text.\n",
    "\n",
    "Decomposition rules:\n",
    "- If the input expresses ONE coherent information need, return exactly 1 refined query.\n",
    "- If the input contains multiple distinct information needs different questions topics or aspects return 2 to 5 sub-queries.\n",
    "- Each sub-query must be atomic one concept or aspect only.\n",
    "- Avoid overlap no near-duplicate sub-queries.\n",
    "\n",
    "Refinement rules apply to every sub-query:\n",
    "- Preserve the user intent exactly do not change meaning.\n",
    "- Do NOT add new concepts not present or clearly implied.\n",
    "- Use precise academic or technical terminology.\n",
    "- Prefer noun phrases avoid questions and full sentences.\n",
    "- Remove filler words stopwords and conversational phrasing.\n",
    "- Length 5 to 12 words per sub-query max 12.\n",
    "- Do NOT use punctuation quotes Boolean operators or special syntax.\n",
    "- Do NOT include years author names or datasets unless explicitly present in the input.\n",
    "- Properly identify distinct aspects if the input is broad or vague.\n",
    "- Do NOT over-split sub-queries unnecessarily.\n",
    "- Try to keep related concepts together.\n",
    "\n",
    "Return the structured output now.\n",
    "\"\"\".format(examples=examples)\n",
    "\n",
    "query_decomposition_model = \"gpt-5-mini\"\n",
    "query_decomposition_description = \"Analyzes research queries and generates optimized, non-overlapping sub-queries for vector database retrieval.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e1530a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_retrieval_prompt = \"\"\"\n",
    "You are a vectorstore retrieval orchestration agent. Your primary responsibility is to retrieve academic context from a vector database using refined sub-queries.\n",
    "\n",
    "Task:\n",
    "You will receive one or more sub-queries from the query decomposition agent. Your role is to use the retrieve_from_vectorstore tool to fetch relevant academic papers, citations, and context for each sub-query.\n",
    "\n",
    "Instructions:\n",
    "1. Accept the list of sub-queries provided by the upstream agent.\n",
    "2. Call retrieve_from_vectorstore with ALL sub-queries at once to retrieve relevant documents and answers from the vector database.\n",
    "3. The tool returns a dictionary mapping each sub-query to its retrieved context and generated answer.\n",
    "4. Aggregate and structure the retrieved results, ensuring no loss of information.\n",
    "5. Present the aggregated retrieval results in a clear, hierarchical format organized by sub-query.\n",
    "\n",
    "Output format:\n",
    "- Organize results by sub-query as keys\n",
    "- Include retrieved context, citations, and synthesized answers for each sub-query\n",
    "- Preserve all citation metadata (source, page, chunk information)\n",
    "- Flag any sub-queries that returned insufficient context\n",
    "\n",
    "Important:\n",
    "- Always use retrieve_from_vectorstore to access the vector database. Do not attempt to retrieve information manually.\n",
    "- Ensure all sub-queries are passed to the tool in a single batch for efficiency.\n",
    "- If retrieval fails for any sub-query, clearly indicate which ones had issues.\n",
    "\"\"\"\n",
    "\n",
    "vectorstore_retrieval_model = \"gpt-5-nano\"\n",
    "vectorstore_retrieval_description = \"Executes batch retrieval of academic context from vector database and preserves citations and metadata for structured result aggregation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b3c35",
   "metadata": {},
   "source": [
    "**Main Deep Agent**\n",
    "- Decomposes user queries into subqueries\n",
    "- Retrieves relevant information from the vectorstore using subqueries\n",
    "- Formats and structures the retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "764383f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from deepagents.middleware.subagents import SubAgentMiddleware\n",
    "from langchain.agents.middleware import TodoListMiddleware\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"gpt-5-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9558bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "class AggregatedContext(TypedDict):\n",
    "    sub_query: Annotated[str, \"The sub-query string\"]\n",
    "    retrieved_context: Annotated[str, \"The retrieved context string\"]\n",
    "    citations: Annotated[List[str], \"List of citation identifiers\"]\n",
    "    synthesized_answer: Annotated[str, \"The synthesized answer string\"]\n",
    "\n",
    "class AggregatedContextList(TypedDict):\n",
    "    results: Annotated[List[AggregatedContext], \"List of aggregated context for each sub-query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6509636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "You are the main retrieval orchestrator for academic question answering. You coordinate sub-agents for query decomposition and vectorstore retrieval, then deliver a concise, citation-backed response.\n",
    "\n",
    "Workflow:\n",
    "1) Send the user query to query_decomposition_subagent. Expect an object with key sub_queries.\n",
    "2) If decomposition fails or returns no sub-queries, fall back to a single sub-query equal to the original user query.\n",
    "3) Send the full sub_queries list to vectorstore_retrieval_subagent (retrieve_from_vectorstore tool) in one batch. Expect a dictionary mapping each sub-query to retrieved context and an answer with citations.\n",
    "4) Aggregate all sub-query answers into a single, coherent response.\n",
    "\n",
    "Response format (strict):\n",
    "- Final answer: 2â€“6 sentences, each factual claim ends with citations from the retrieved context.\n",
    "- Key points: 3â€“7 bullets, each bullet ends with citations.\n",
    "- Assumptions: list assumptions made, or \"None\".\n",
    "\n",
    "Rules:\n",
    "- Never invent citations; only use those provided by the retrieval results.\n",
    "- If any sub-query returned insufficient context, say so explicitly for that sub-query.\n",
    "- If no context supports an answer, reply: \"Not answerable from context.\".\n",
    "- Keep wording concise and academic; avoid markdown tables and superfluous formatting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e20205",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=PROMPT,\n",
    "    middleware=[\n",
    "        TodoListMiddleware(\n",
    "            system_prompt=\n",
    "            \"\"\"\n",
    "                Always use the write_todos tool to break down the user's academic query into the following sequential steps:\n",
    "        \n",
    "                1. DECOMPOSE: Analyze the user's query and identify distinct information needs. Send to query_decomposition_subagent to generate refined sub-queries.\n",
    "                2. VALIDATE: Ensure the decomposed sub-queries are non-overlapping and atomic (each addresses one concept/aspect).\n",
    "                3. RETRIEVE: Pass all sub-queries to vectorstore_retrieval_subagent in a single batch to fetch relevant academic context with citations.\n",
    "                4. AGGREGATE: Organize the retrieved results hierarchically by sub-query, preserving all citation metadata.\n",
    "                5. SYNTHESIZE: Combine individual answers into a cohesive final response with citations, key points, and assumptions.\n",
    "                6. REVIEW: Verify all factual claims are backed by citations from the retrieved context.\n",
    "                \n",
    "                Complete each step before moving to the next. If any step fails, flag the issue explicitly.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        SubAgentMiddleware(\n",
    "            default_model=\"gpt-4o\",\n",
    "            default_tools=[],\n",
    "            subagents=[\n",
    "                {\n",
    "                    \"name\": \"query_decomposition_subagent\",\n",
    "                    \"description\": query_decomposition_description,\n",
    "                    \"system_prompt\": query_decomposition_prompt,\n",
    "                    \"model\": query_decomposition_model,\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"vectorstore_retrieval_subagent\",\n",
    "                    \"description\": vectorstore_retrieval_description,\n",
    "                    \"system_prompt\": vectorstore_retrieval_prompt,\n",
    "                    \"tools\": [retrieve_from_vectorstore],\n",
    "                    \"model\": vectorstore_retrieval_model,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    "    response_format=AggregatedContextList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0493c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 10:58:41,659 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 10:58:44,641 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 10:58:57,148 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 10:59:01,160 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 10:59:05,737 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 10:59:11,833 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 10:59:13,126 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 10:59:17,530 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 10:59:19,139 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 10:59:23,641 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 11:00:37,625 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 11:00:41,834 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 11:01:21,043 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the self attention mechanism and how does it work in transformer models?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ebdfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sub_query': 'self attention mechanism definition and purpose',\n",
       "  'retrieved_context': 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence (Vaswani et al., 2017, Section 2). The Transformer uses self-attention in encoder and decoder layers to allow each position to attend to all positions in the previous layer, enabling modeling of dependencies without recurrence (Vaswani et al., 2017, Section 3.2.3). Self-attention connects all positions with a constant number of sequential operations, improving parallelization and shortening path lengths for long-range dependencies compared to recurrent layers (Vaswani et al., 2017, Section 4).',\n",
       "  'citations': ['Vaswani et al., 2017 - Attention Is All You Need; Section 2, 3.2.3, 4; https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'],\n",
       "  'synthesized_answer': 'Self-attention (intra-attention) relates positions within a single sequence to compute contextualized representations, enabling the model to represent each token with information from all other tokens in the sequence [Vaswani et al., 2017, Section 2].'},\n",
       " {'sub_query': 'self attention operation within transformer architecture',\n",
       "  'retrieved_context': 'In a self-attention layer all keys, values and queries come from the same source (the previous layer) and each position can attend to all positions in that layer; in the decoder self-attention is masked to prevent leftward (future) information flow and the model also uses encoder-decoder attention where decoder queries attend encoder keys/values (Vaswani et al., 2017, Section 3.2.3). The Transformer implements multi-head attention and scaled dot-product attention to compute weights and aggregate values, enabling parallel computation and flexible representation learning (Vaswani et al., 2017, Sections 3.2.3 and 4).',\n",
       "  'citations': ['Vaswani et al., 2017 - Attention Is All You Need; Section 3.2.3, 4; https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'],\n",
       "  'synthesized_answer': 'Transformer self-attention forms queries, keys, and values from the same input, computes attention weights (e.g., scaled dot-product, often via multiple heads), applies those weights to values to produce context-aware outputs, uses masking in decoder self-attention to preserve autoregression, and includes encoderâ€“decoder attention to let the decoder attend to encoder outputs [Vaswani et al., 2017, Sections 3.2.3 and 4].'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"structured_response\"]['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7515d0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task\n"
     ]
    }
   ],
   "source": [
    "for tool in result['messages'][3].tool_calls:\n",
    "    print(tool['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde49c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â”‚ ðŸ¤– Agent Execution Summary â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[1;36mâ”‚\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36mðŸ¤– Agent Execution Summary\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36mâ”‚\u001b[0m\n",
       "\u001b[1;36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">================================================================================\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "================================================================================\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 1: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">What is the self attention mechanism and how does ...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 1: \u001b[0m\u001b[1;36mWhat is the self attention mechanism and how does ...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">What is the self attention mechanism and how does it work in transformer models?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mWhat is the self attention mechanism and how does it work in transformer models?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 2: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 2: \u001b[0m\u001b[1;36mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 3: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Updated todo list to [{'content': \"DECOMPOSE: Send...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 3: \u001b[0m\u001b[1;36mUpdated todo list to [{'content': \"DECOMPOSE: Send...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Updated todo list to [{'content': \"DECOMPOSE: Send user's query to query_decomposition_subagent to generate </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sub-queries\", 'status': 'in_progress'}, {'content': 'VALIDATE: Ensure sub-queries are atomic and non-overlapping', </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">'status': 'pending'}, {'content': 'RETRIEVE: Send sub-queries list in one batch to vectorstore_retrieval_subagent </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">to fetch context and citations', 'status': 'pending'}, {'content': 'AGGREGATE: Organize retrieved results by </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sub-query preserving citations', 'status': 'pending'}, {'content': 'SYNTHESIZE: Combine sub-query answers into </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">final response with citations, key points, and assumptions', 'status': 'pending'}, {'content': 'REVIEW: Verify all </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">claims are backed by retrieved citations and mark completion', 'status': 'pending'}]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mUpdated todo list to [{'content': \"DECOMPOSE: Send user's query to query_decomposition_subagent to generate \u001b[0m\n",
       "\u001b[2;37msub-queries\", 'status': 'in_progress'}, {'content': 'VALIDATE: Ensure sub-queries are atomic and non-overlapping', \u001b[0m\n",
       "\u001b[2;37m'status': 'pending'}, {'content': 'RETRIEVE: Send sub-queries list in one batch to vectorstore_retrieval_subagent \u001b[0m\n",
       "\u001b[2;37mto fetch context and citations', 'status': 'pending'}, {'content': 'AGGREGATE: Organize retrieved results by \u001b[0m\n",
       "\u001b[2;37msub-query preserving citations', 'status': 'pending'}, {'content': 'SYNTHESIZE: Combine sub-query answers into \u001b[0m\n",
       "\u001b[2;37mfinal response with citations, key points, and assumptions', 'status': 'pending'}, {'content': 'REVIEW: Verify all \u001b[0m\n",
       "\u001b[2;37mclaims are backed by retrieved citations and mark completion', 'status': 'pending'}]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 4: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tool Calls: task</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 4: \u001b[0m\u001b[1;36mTool Calls: task\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Tool Calls: task</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mTool Calls: task\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 5: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">{\"sub_queries\":[\"self attention mechanism definiti...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 5: \u001b[0m\u001b[1;36m{\"sub_queries\":[\"self attention mechanism definiti...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">{\"sub_queries\":[\"self attention mechanism definition and purpose\",\"self attention operation within transformer </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">architecture\"]}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37m{\"sub_queries\":[\"self attention mechanism definition and purpose\",\"self attention operation within transformer \u001b[0m\n",
       "\u001b[2;37marchitecture\"]}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 6: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 6: \u001b[0m\u001b[1;36mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 7: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Updated todo list to [{'content': \"DECOMPOSE: Send...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 7: \u001b[0m\u001b[1;36mUpdated todo list to [{'content': \"DECOMPOSE: Send...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Updated todo list to [{'content': \"DECOMPOSE: Send user's query to query_decomposition_subagent to generate </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sub-queries\", 'status': 'completed'}, {'content': 'VALIDATE: Ensure sub-queries are atomic and non-overlapping', </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">'status': 'in_progress'}, {'content': 'RETRIEVE: Send sub-queries list in one batch to </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">vectorstore_retrieval_subagent to fetch context and citations', 'status': 'pending'}, {'content': 'AGGREGATE: </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Organize retrieved results by sub-query preserving citations', 'status': 'pending'}, {'content': 'SYNTHESIZE: </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Combine sub-query answers into final response with citations, key points, and assumptions', 'status': 'pending'}, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">{'content': 'REVIEW: Verify all claims are backed by retrieved citations and mark completion', 'status': </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">'pending'}]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mUpdated todo list to [{'content': \"DECOMPOSE: Send user's query to query_decomposition_subagent to generate \u001b[0m\n",
       "\u001b[2;37msub-queries\", 'status': 'completed'}, {'content': 'VALIDATE: Ensure sub-queries are atomic and non-overlapping', \u001b[0m\n",
       "\u001b[2;37m'status': 'in_progress'}, {'content': 'RETRIEVE: Send sub-queries list in one batch to \u001b[0m\n",
       "\u001b[2;37mvectorstore_retrieval_subagent to fetch context and citations', 'status': 'pending'}, {'content': 'AGGREGATE: \u001b[0m\n",
       "\u001b[2;37mOrganize retrieved results by sub-query preserving citations', 'status': 'pending'}, {'content': 'SYNTHESIZE: \u001b[0m\n",
       "\u001b[2;37mCombine sub-query answers into final response with citations, key points, and assumptions', 'status': 'pending'}, \u001b[0m\n",
       "\u001b[2;37m{'content': 'REVIEW: Verify all claims are backed by retrieved citations and mark completion', 'status': \u001b[0m\n",
       "\u001b[2;37m'pending'}]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 8: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tool Calls: task</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 8: \u001b[0m\u001b[1;36mTool Calls: task\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Tool Calls: task</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mTool Calls: task\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 9: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">{</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">  \"self attention mechanism definition and purpo...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 9: \u001b[0m\u001b[1;36m{\u001b[0m\n",
       "\u001b[1;36m  \"self attention mechanism definition and purpo...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">{</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">  \"self attention mechanism definition and purpose\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    \"retrieved_context\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"passage\": \"Self-attention, sometimes called intra-attention is an attention mechanism relating different </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">positions of a single sequence in order to compute a representation of the sequence.\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 2,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"2 Background\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"passage\": \"The Transformer uses multi-head attention in three different ways: - In \\\"encoder-decoder </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">attention\\\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">output of the encoder. This allows every position in the decoder to attend over all positions in the input </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">2, 8]. - The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">come from the same place, in this case, the output of the previous layer in the encoder. Each position in the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">encoder can attend to all positions in the previous layer of the encoder. - Similarly, self-attention layers in the</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">decoder allow each position in the decoder to attend to all positions in the decoder up to and including that </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">implement this inside of scaled dot-product attention by masking out (setting to -âˆž ) all values in the input of </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">the softmax which correspond to illegal connections. See Figure 2.\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 3,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"3.2.3 Applications of Attention in our Model\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"passage\": \"In this section we compare various aspects of self-attention layers to the recurrent and </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">convolutional layers commonly used for mapping one variable-length sequence of symbol representations ( x 1 , ..., </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">x n ) to another sequence of equal length ( z 1 , ..., z n ), with x i , z i âˆˆ R d , such as a hidden layer in a </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">parallelized, as measured by the minimum number of sequential operations required. The third is the path length </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">paths forward and backward signals have to traverse in the network. The shorter these paths between any combination</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">also compare the maximum path length between any two input and output positions in networks composed of the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequentially executed operations, whereas a recurrent layer requires O ( n ) sequential operations. In terms of </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">smaller than the representation dimensionality d , which is most often the case with sentence representations used </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">improve computational performance for tasks involving very long sequences, self-attention could be restricted to </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">considering only a neighborhood of size r in the input sequence centered around the respective output position. </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">This would increase the maximum path length to O ( n/r ) . We plan to investigate this approach further in future </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">work.\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 4,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"4 Why Self-Attention\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    \"citations\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": \"Vaswani et al., 2017\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"title\": \"Attention Is All You Need\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"2 Background\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 2,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"url\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": \"Vaswani et al., 2017\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"title\": \"Attention Is All You Need\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"3.2.3 Applications of Attention in our Model\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 3,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"url\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": \"Vaswani et al., 2017\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"title\": \"Attention Is All You Need\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"4 Why Self-Attention\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 4,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"url\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    \"synthesized_answer\": \"Self-attention (intra-attention) is an attention mechanism that relates different </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">positions of a single sequence to compute a representation of that sequence. It enables the Transformer to connect </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">all positions with a constant number of operations, enabling parallel computation and effective modeling of </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">long-range dependencies. In the Transformer, self-attention is used in both the encoder (self-attention over the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">input) and the decoder (self-attention over the generated representation, with masking to prevent attending to </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">future positions), plus encoder-decoder attention that lets the decoder attend to encoder outputs. This </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">architecture eliminates the need for recurrent or convolutional components for capturing dependencies.\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">  },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">  \"self attention operation within transformer architecture\": {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    \"retrieved_context\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"passage\": \"Self-attention, sometimes called intra-attention is an attention mechanism relating different </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">positions of a single sequence in order to compute a representation of the sequence.\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 2,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"2 Background\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"passage\": \"The Transformer uses multi-head attention in three different ways: - In \\\"encoder-decoder </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">attention\\\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">output of the encoder. This allows every position in the decoder to attend over all positions in the input </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">2, 8]. - The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">come from the same place, in this case, the output of the previous layer in the encoder. Each position in the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">encoder can attend to all positions in the previous layer of the encoder. - Similarly, self-attention layers in the</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">decoder allow each position in the decoder to attend to all positions in the decoder up to and including that </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">implement this inside of scaled dot-product attention by masking out (setting to -âˆž ) all values in the input of </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">the softmax which correspond to illegal connections. See Figure 2.\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 3,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"3.2.3 Applications of Attention in our Model\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"passage\": \"In this section we compare various aspects of self-attention layers to the recurrent and </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">convolutional layers commonly used for mapping one variable-length sequence of symbol representations ( x 1 , ..., </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">x n ) to another sequence of equal length ( z 1 , ..., z n ), with x i , z i âˆˆ R d , such as a hidden layer in a </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">parallelized, as measured by the minimum number of sequential operations required. The third is the path length </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">paths forward and backward signals have to traverse in the network. The shorter these paths between any combination</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">also compare the maximum path length between any two input and output positions in networks composed of the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sequentially executed operations, whereas a recurrent layer requires O ( n ) sequential operations. In terms of </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">smaller than the representation dimensionality d , which is most often the case with sentence representations used </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">improve computational performance for tasks involving very long sequences, self-attention could be restricted to </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">considering only a neighborhood of size r in the input sequence centered around the respective output position. </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">This would increase the maximum path length to O ( n/r ) . We plan to investigate this approach further in future </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">work.\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 4,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"4 Why Self-Attention\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    \"citations\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": \"Vaswani et al., 2017\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"title\": \"Attention Is All You Need\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"2 Background\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 2,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"url\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": \"Vaswani et al., 2017\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"title\": \"Attention Is All You Need\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"3.2.3 Applications of Attention in our Model\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 3,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"url\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"source\": \"Vaswani et al., 2017\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"title\": \"Attention Is All You Need\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"section\": \"4 Why Self-Attention\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"page\": 4,</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"url\": </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    \"synthesized_answer\": \"The self-attention operation in the Transformer computes attention over the positions </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">within a sequence by forming queries, keys, and values from the same input representation; the attention weights </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">are computed in a way that emphasizes relevant positions and is applied to the values, enabling the model to </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">aggregate contextual information from all positions. In the encoder, Q, K, and V come from the previous encoder </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">layer; in the decoder, self-attention is masked to prevent attending to future positions, preserving </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">autoregression; there is also encoder-decoder attention that connects encoder outputs to decoder queries. This </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">architecture enables global dependency modeling and high parallelization without recurrence or convolution.\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">  }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37m{\u001b[0m\n",
       "\u001b[2;37m  \"self attention mechanism definition and purpose\": {\u001b[0m\n",
       "\u001b[2;37m    \"retrieved_context\": [\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"passage\": \"Self-attention, sometimes called intra-attention is an attention mechanism relating different \u001b[0m\n",
       "\u001b[2;37mpositions of a single sequence in order to compute a representation of the sequence.\",\u001b[0m\n",
       "\u001b[2;37m        \"source\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 2,\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"2 Background\"\u001b[0m\n",
       "\u001b[2;37m      },\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"passage\": \"The Transformer uses multi-head attention in three different ways: - In \\\"encoder-decoder \u001b[0m\n",
       "\u001b[2;37mattention\\\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the \u001b[0m\n",
       "\u001b[2;37moutput of the encoder. This allows every position in the decoder to attend over all positions in the input \u001b[0m\n",
       "\u001b[2;37msequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, \u001b[0m\n",
       "\u001b[2;37m2, 8]. - The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries \u001b[0m\n",
       "\u001b[2;37mcome from the same place, in this case, the output of the previous layer in the encoder. Each position in the \u001b[0m\n",
       "\u001b[2;37mencoder can attend to all positions in the previous layer of the encoder. - Similarly, self-attention layers in the\u001b[0m\n",
       "\u001b[2;37mdecoder allow each position in the decoder to attend to all positions in the decoder up to and including that \u001b[0m\n",
       "\u001b[2;37mposition. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We \u001b[0m\n",
       "\u001b[2;37mimplement this inside of scaled dot-product attention by masking out (setting to -âˆž ) all values in the input of \u001b[0m\n",
       "\u001b[2;37mthe softmax which correspond to illegal connections. See Figure 2.\",\u001b[0m\n",
       "\u001b[2;37m        \"source\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 3,\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"3.2.3 Applications of Attention in our Model\"\u001b[0m\n",
       "\u001b[2;37m      },\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"passage\": \"In this section we compare various aspects of self-attention layers to the recurrent and \u001b[0m\n",
       "\u001b[2;37mconvolutional layers commonly used for mapping one variable-length sequence of symbol representations ( x 1 , ..., \u001b[0m\n",
       "\u001b[2;37mx n ) to another sequence of equal length ( z 1 , ..., z n ), with x i , z i âˆˆ R d , such as a hidden layer in a \u001b[0m\n",
       "\u001b[2;37mtypical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three \u001b[0m\n",
       "\u001b[2;37mdesiderata. One is the total computational complexity per layer. Another is the amount of computation that can be \u001b[0m\n",
       "\u001b[2;37mparallelized, as measured by the minimum number of sequential operations required. The third is the path length \u001b[0m\n",
       "\u001b[2;37mbetween long-range dependencies in the network. Learning long-range dependencies is a key challenge in many \u001b[0m\n",
       "\u001b[2;37msequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the \u001b[0m\n",
       "\u001b[2;37mpaths forward and backward signals have to traverse in the network. The shorter these paths between any combination\u001b[0m\n",
       "\u001b[2;37mof positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we \u001b[0m\n",
       "\u001b[2;37malso compare the maximum path length between any two input and output positions in networks composed of the \u001b[0m\n",
       "\u001b[2;37mdifferent layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of\u001b[0m\n",
       "\u001b[2;37msequentially executed operations, whereas a recurrent layer requires O ( n ) sequential operations. In terms of \u001b[0m\n",
       "\u001b[2;37mcomputational complexity, self-attention layers are faster than recurrent layers when the sequence length n is \u001b[0m\n",
       "\u001b[2;37msmaller than the representation dimensionality d , which is most often the case with sentence representations used \u001b[0m\n",
       "\u001b[2;37mby state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To \u001b[0m\n",
       "\u001b[2;37mimprove computational performance for tasks involving very long sequences, self-attention could be restricted to \u001b[0m\n",
       "\u001b[2;37mconsidering only a neighborhood of size r in the input sequence centered around the respective output position. \u001b[0m\n",
       "\u001b[2;37mThis would increase the maximum path length to O ( n/r ) . We plan to investigate this approach further in future \u001b[0m\n",
       "\u001b[2;37mwork.\",\u001b[0m\n",
       "\u001b[2;37m        \"source\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 4,\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"4 Why Self-Attention\"\u001b[0m\n",
       "\u001b[2;37m      }\u001b[0m\n",
       "\u001b[2;37m    ],\u001b[0m\n",
       "\u001b[2;37m    \"citations\": [\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"source\": \"Vaswani et al., 2017\",\u001b[0m\n",
       "\u001b[2;37m        \"title\": \"Attention Is All You Need\",\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"2 Background\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 2,\u001b[0m\n",
       "\u001b[2;37m        \"url\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\u001b[0m\n",
       "\u001b[2;37m      },\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"source\": \"Vaswani et al., 2017\",\u001b[0m\n",
       "\u001b[2;37m        \"title\": \"Attention Is All You Need\",\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"3.2.3 Applications of Attention in our Model\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 3,\u001b[0m\n",
       "\u001b[2;37m        \"url\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\u001b[0m\n",
       "\u001b[2;37m      },\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"source\": \"Vaswani et al., 2017\",\u001b[0m\n",
       "\u001b[2;37m        \"title\": \"Attention Is All You Need\",\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"4 Why Self-Attention\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 4,\u001b[0m\n",
       "\u001b[2;37m        \"url\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\u001b[0m\n",
       "\u001b[2;37m      }\u001b[0m\n",
       "\u001b[2;37m    ],\u001b[0m\n",
       "\u001b[2;37m    \"synthesized_answer\": \"Self-attention (intra-attention) is an attention mechanism that relates different \u001b[0m\n",
       "\u001b[2;37mpositions of a single sequence to compute a representation of that sequence. It enables the Transformer to connect \u001b[0m\n",
       "\u001b[2;37mall positions with a constant number of operations, enabling parallel computation and effective modeling of \u001b[0m\n",
       "\u001b[2;37mlong-range dependencies. In the Transformer, self-attention is used in both the encoder (self-attention over the \u001b[0m\n",
       "\u001b[2;37minput) and the decoder (self-attention over the generated representation, with masking to prevent attending to \u001b[0m\n",
       "\u001b[2;37mfuture positions), plus encoder-decoder attention that lets the decoder attend to encoder outputs. This \u001b[0m\n",
       "\u001b[2;37marchitecture eliminates the need for recurrent or convolutional components for capturing dependencies.\"\u001b[0m\n",
       "\u001b[2;37m  },\u001b[0m\n",
       "\u001b[2;37m  \"self attention operation within transformer architecture\": {\u001b[0m\n",
       "\u001b[2;37m    \"retrieved_context\": [\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"passage\": \"Self-attention, sometimes called intra-attention is an attention mechanism relating different \u001b[0m\n",
       "\u001b[2;37mpositions of a single sequence in order to compute a representation of the sequence.\",\u001b[0m\n",
       "\u001b[2;37m        \"source\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 2,\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"2 Background\"\u001b[0m\n",
       "\u001b[2;37m      },\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"passage\": \"The Transformer uses multi-head attention in three different ways: - In \\\"encoder-decoder \u001b[0m\n",
       "\u001b[2;37mattention\\\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the \u001b[0m\n",
       "\u001b[2;37moutput of the encoder. This allows every position in the decoder to attend over all positions in the input \u001b[0m\n",
       "\u001b[2;37msequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, \u001b[0m\n",
       "\u001b[2;37m2, 8]. - The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries \u001b[0m\n",
       "\u001b[2;37mcome from the same place, in this case, the output of the previous layer in the encoder. Each position in the \u001b[0m\n",
       "\u001b[2;37mencoder can attend to all positions in the previous layer of the encoder. - Similarly, self-attention layers in the\u001b[0m\n",
       "\u001b[2;37mdecoder allow each position in the decoder to attend to all positions in the decoder up to and including that \u001b[0m\n",
       "\u001b[2;37mposition. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We \u001b[0m\n",
       "\u001b[2;37mimplement this inside of scaled dot-product attention by masking out (setting to -âˆž ) all values in the input of \u001b[0m\n",
       "\u001b[2;37mthe softmax which correspond to illegal connections. See Figure 2.\",\u001b[0m\n",
       "\u001b[2;37m        \"source\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 3,\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"3.2.3 Applications of Attention in our Model\"\u001b[0m\n",
       "\u001b[2;37m      },\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"passage\": \"In this section we compare various aspects of self-attention layers to the recurrent and \u001b[0m\n",
       "\u001b[2;37mconvolutional layers commonly used for mapping one variable-length sequence of symbol representations ( x 1 , ..., \u001b[0m\n",
       "\u001b[2;37mx n ) to another sequence of equal length ( z 1 , ..., z n ), with x i , z i âˆˆ R d , such as a hidden layer in a \u001b[0m\n",
       "\u001b[2;37mtypical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three \u001b[0m\n",
       "\u001b[2;37mdesiderata. One is the total computational complexity per layer. Another is the amount of computation that can be \u001b[0m\n",
       "\u001b[2;37mparallelized, as measured by the minimum number of sequential operations required. The third is the path length \u001b[0m\n",
       "\u001b[2;37mbetween long-range dependencies in the network. Learning long-range dependencies is a key challenge in many \u001b[0m\n",
       "\u001b[2;37msequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the \u001b[0m\n",
       "\u001b[2;37mpaths forward and backward signals have to traverse in the network. The shorter these paths between any combination\u001b[0m\n",
       "\u001b[2;37mof positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we \u001b[0m\n",
       "\u001b[2;37malso compare the maximum path length between any two input and output positions in networks composed of the \u001b[0m\n",
       "\u001b[2;37mdifferent layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of\u001b[0m\n",
       "\u001b[2;37msequentially executed operations, whereas a recurrent layer requires O ( n ) sequential operations. In terms of \u001b[0m\n",
       "\u001b[2;37mcomputational complexity, self-attention layers are faster than recurrent layers when the sequence length n is \u001b[0m\n",
       "\u001b[2;37msmaller than the representation dimensionality d , which is most often the case with sentence representations used \u001b[0m\n",
       "\u001b[2;37mby state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To \u001b[0m\n",
       "\u001b[2;37mimprove computational performance for tasks involving very long sequences, self-attention could be restricted to \u001b[0m\n",
       "\u001b[2;37mconsidering only a neighborhood of size r in the input sequence centered around the respective output position. \u001b[0m\n",
       "\u001b[2;37mThis would increase the maximum path length to O ( n/r ) . We plan to investigate this approach further in future \u001b[0m\n",
       "\u001b[2;37mwork.\",\u001b[0m\n",
       "\u001b[2;37m        \"source\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 4,\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"4 Why Self-Attention\"\u001b[0m\n",
       "\u001b[2;37m      }\u001b[0m\n",
       "\u001b[2;37m    ],\u001b[0m\n",
       "\u001b[2;37m    \"citations\": [\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"source\": \"Vaswani et al., 2017\",\u001b[0m\n",
       "\u001b[2;37m        \"title\": \"Attention Is All You Need\",\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"2 Background\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 2,\u001b[0m\n",
       "\u001b[2;37m        \"url\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\u001b[0m\n",
       "\u001b[2;37m      },\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"source\": \"Vaswani et al., 2017\",\u001b[0m\n",
       "\u001b[2;37m        \"title\": \"Attention Is All You Need\",\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"3.2.3 Applications of Attention in our Model\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 3,\u001b[0m\n",
       "\u001b[2;37m        \"url\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\u001b[0m\n",
       "\u001b[2;37m      },\u001b[0m\n",
       "\u001b[2;37m      {\u001b[0m\n",
       "\u001b[2;37m        \"source\": \"Vaswani et al., 2017\",\u001b[0m\n",
       "\u001b[2;37m        \"title\": \"Attention Is All You Need\",\u001b[0m\n",
       "\u001b[2;37m        \"section\": \"4 Why Self-Attention\",\u001b[0m\n",
       "\u001b[2;37m        \"page\": 4,\u001b[0m\n",
       "\u001b[2;37m        \"url\": \u001b[0m\n",
       "\u001b[2;37m\"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\u001b[0m\n",
       "\u001b[2;37m      }\u001b[0m\n",
       "\u001b[2;37m    ],\u001b[0m\n",
       "\u001b[2;37m    \"synthesized_answer\": \"The self-attention operation in the Transformer computes attention over the positions \u001b[0m\n",
       "\u001b[2;37mwithin a sequence by forming queries, keys, and values from the same input representation; the attention weights \u001b[0m\n",
       "\u001b[2;37mare computed in a way that emphasizes relevant positions and is applied to the values, enabling the model to \u001b[0m\n",
       "\u001b[2;37maggregate contextual information from all positions. In the encoder, Q, K, and V come from the previous encoder \u001b[0m\n",
       "\u001b[2;37mlayer; in the decoder, self-attention is masked to prevent attending to future positions, preserving \u001b[0m\n",
       "\u001b[2;37mautoregression; there is also encoder-decoder attention that connects encoder outputs to decoder queries. This \u001b[0m\n",
       "\u001b[2;37marchitecture enables global dependency modeling and high parallelization without recurrence or convolution.\"\u001b[0m\n",
       "\u001b[2;37m  }\u001b[0m\n",
       "\u001b[2;37m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 10: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 10: \u001b[0m\u001b[1;36mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Tool Calls: write_todos</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mTool Calls: write_todos\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 11: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Updated todo list to [{'content': \"DECOMPOSE: Send...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 11: \u001b[0m\u001b[1;36mUpdated todo list to [{'content': \"DECOMPOSE: Send...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">Updated todo list to [{'content': \"DECOMPOSE: Send user's query to query_decomposition_subagent to generate </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sub-queries\", 'status': 'completed'}, {'content': 'VALIDATE: Ensure sub-queries are atomic and non-overlapping', </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">'status': 'completed'}, {'content': 'RETRIEVE: Send sub-queries list in one batch to vectorstore_retrieval_subagent</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">to fetch context and citations', 'status': 'completed'}, {'content': 'AGGREGATE: Organize retrieved results by </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">sub-query preserving citations', 'status': 'in_progress'}, {'content': 'SYNTHESIZE: Combine sub-query answers into </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">final response with citations, key points, and assumptions', 'status': 'pending'}, {'content': 'REVIEW: Verify all </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">claims are backed by retrieved citations and mark completion', 'status': 'pending'}]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37mUpdated todo list to [{'content': \"DECOMPOSE: Send user's query to query_decomposition_subagent to generate \u001b[0m\n",
       "\u001b[2;37msub-queries\", 'status': 'completed'}, {'content': 'VALIDATE: Ensure sub-queries are atomic and non-overlapping', \u001b[0m\n",
       "\u001b[2;37m'status': 'completed'}, {'content': 'RETRIEVE: Send sub-queries list in one batch to vectorstore_retrieval_subagent\u001b[0m\n",
       "\u001b[2;37mto fetch context and citations', 'status': 'completed'}, {'content': 'AGGREGATE: Organize retrieved results by \u001b[0m\n",
       "\u001b[2;37msub-query preserving citations', 'status': 'in_progress'}, {'content': 'SYNTHESIZE: Combine sub-query answers into \u001b[0m\n",
       "\u001b[2;37mfinal response with citations, key points, and assumptions', 'status': 'pending'}, {'content': 'REVIEW: Verify all \u001b[0m\n",
       "\u001b[2;37mclaims are backed by retrieved citations and mark completion', 'status': 'pending'}]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Step 12: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">{</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">  \"results\": [</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">    {</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">      \"sub_query\": \"self at...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mStep 12: \u001b[0m\u001b[1;36m{\u001b[0m\n",
       "\u001b[1;36m  \"results\": [\u001b[0m\n",
       "\u001b[1;36m    {\u001b[0m\n",
       "\u001b[1;36m      \"sub_query\": \"self at...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">{</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">  \"results\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      \"sub_query\": \"self attention mechanism definition and purpose\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      \"retrieved_context\": \"Self-attention, sometimes called intra-attention is an attention mechanism relating </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">different positions of a single sequence in order to compute a representation of the sequence (Vaswani et al., </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">2017, Section 2). The Transformer uses self-attention in encoder and decoder layers to allow each position to </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">attend to all positions in the previous layer, enabling modeling of dependencies without recurrence (Vaswani et </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">al., 2017, Section 3.2.3). Self-attention connects all positions with a constant number of sequential operations, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">improving parallelization and shortening path lengths for long-range dependencies compared to recurrent layers </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">(Vaswani et al., 2017, Section 4).\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      \"citations\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"Vaswani et al., 2017 - Attention Is All You Need; Section 2, 3.2.3, 4; </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      \"synthesized_answer\": \"Self-attention (intra-attention) relates positions within a single sequence to compute</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">contextualized representations, enabling the model to represent each token with information from all other tokens </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">in the sequence [Vaswani et al., 2017, Section 2].\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    },</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    {</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      \"sub_query\": \"self attention operation within transformer architecture\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      \"retrieved_context\": \"In a self-attention layer all keys, values and queries come from the same source (the </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">previous layer) and each position can attend to all positions in that layer; in the decoder self-attention is </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">masked to prevent leftward (future) information flow and the model also uses encoder-decoder attention where </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">decoder queries attend encoder keys/values (Vaswani et al., 2017, Section 3.2.3). The Transformer implements </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">multi-head attention and scaled dot-product attention to compute weights and aggregate values, enabling parallel </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">computation and flexible representation learning (Vaswani et al., 2017, Sections 3.2.3 and 4).\",</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      \"citations\": [</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">        \"Vaswani et al., 2017 - Attention Is All You Need; Section 3.2.3, 4; </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      ],</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">      \"synthesized_answer\": \"Transformer self-attention forms queries, keys, and values from the same input, </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">computes attention weights (e.g., scaled dot-product, often via multiple heads), applies those weights to values to</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">produce context-aware outputs, uses masking in decoder self-attention to preserve autoregression, and includes </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">encoderâ€“decoder attention to let the decoder attend to encoder outputs [Vaswani et al., 2017, Sections 3.2.3 and </span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">4].\"</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">    }</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">  ]</span>\n",
       "<span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;37m{\u001b[0m\n",
       "\u001b[2;37m  \"results\": [\u001b[0m\n",
       "\u001b[2;37m    {\u001b[0m\n",
       "\u001b[2;37m      \"sub_query\": \"self attention mechanism definition and purpose\",\u001b[0m\n",
       "\u001b[2;37m      \"retrieved_context\": \"Self-attention, sometimes called intra-attention is an attention mechanism relating \u001b[0m\n",
       "\u001b[2;37mdifferent positions of a single sequence in order to compute a representation of the sequence (Vaswani et al., \u001b[0m\n",
       "\u001b[2;37m2017, Section 2). The Transformer uses self-attention in encoder and decoder layers to allow each position to \u001b[0m\n",
       "\u001b[2;37mattend to all positions in the previous layer, enabling modeling of dependencies without recurrence (Vaswani et \u001b[0m\n",
       "\u001b[2;37mal., 2017, Section 3.2.3). Self-attention connects all positions with a constant number of sequential operations, \u001b[0m\n",
       "\u001b[2;37mimproving parallelization and shortening path lengths for long-range dependencies compared to recurrent layers \u001b[0m\n",
       "\u001b[2;37m(Vaswani et al., 2017, Section 4).\",\u001b[0m\n",
       "\u001b[2;37m      \"citations\": [\u001b[0m\n",
       "\u001b[2;37m        \"Vaswani et al., 2017 - Attention Is All You Need; Section 2, 3.2.3, 4; \u001b[0m\n",
       "\u001b[2;37mhttps://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\u001b[0m\n",
       "\u001b[2;37m      ],\u001b[0m\n",
       "\u001b[2;37m      \"synthesized_answer\": \"Self-attention (intra-attention) relates positions within a single sequence to compute\u001b[0m\n",
       "\u001b[2;37mcontextualized representations, enabling the model to represent each token with information from all other tokens \u001b[0m\n",
       "\u001b[2;37min the sequence [Vaswani et al., 2017, Section 2].\"\u001b[0m\n",
       "\u001b[2;37m    },\u001b[0m\n",
       "\u001b[2;37m    {\u001b[0m\n",
       "\u001b[2;37m      \"sub_query\": \"self attention operation within transformer architecture\",\u001b[0m\n",
       "\u001b[2;37m      \"retrieved_context\": \"In a self-attention layer all keys, values and queries come from the same source (the \u001b[0m\n",
       "\u001b[2;37mprevious layer) and each position can attend to all positions in that layer; in the decoder self-attention is \u001b[0m\n",
       "\u001b[2;37mmasked to prevent leftward (future) information flow and the model also uses encoder-decoder attention where \u001b[0m\n",
       "\u001b[2;37mdecoder queries attend encoder keys/values (Vaswani et al., 2017, Section 3.2.3). The Transformer implements \u001b[0m\n",
       "\u001b[2;37mmulti-head attention and scaled dot-product attention to compute weights and aggregate values, enabling parallel \u001b[0m\n",
       "\u001b[2;37mcomputation and flexible representation learning (Vaswani et al., 2017, Sections 3.2.3 and 4).\",\u001b[0m\n",
       "\u001b[2;37m      \"citations\": [\u001b[0m\n",
       "\u001b[2;37m        \"Vaswani et al., 2017 - Attention Is All You Need; Section 3.2.3, 4; \u001b[0m\n",
       "\u001b[2;37mhttps://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\u001b[0m\n",
       "\u001b[2;37m      ],\u001b[0m\n",
       "\u001b[2;37m      \"synthesized_answer\": \"Transformer self-attention forms queries, keys, and values from the same input, \u001b[0m\n",
       "\u001b[2;37mcomputes attention weights (e.g., scaled dot-product, often via multiple heads), applies those weights to values to\u001b[0m\n",
       "\u001b[2;37mproduce context-aware outputs, uses masking in decoder self-attention to preserve autoregression, and includes \u001b[0m\n",
       "\u001b[2;37mencoderâ€“decoder attention to let the decoder attend to encoder outputs [Vaswani et al., 2017, Sections 3.2.3 and \u001b[0m\n",
       "\u001b[2;37m4].\"\u001b[0m\n",
       "\u001b[2;37m    }\u001b[0m\n",
       "\u001b[2;37m  ]\u001b[0m\n",
       "\u001b[2;37m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â”‚ ðŸ“Š Retrieved Context &amp; Answers â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\u001b[0m\n",
       "\u001b[1;32mâ”‚\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32mðŸ“Š Retrieved Context & Answers\u001b[0m\u001b[1;32m \u001b[0m\u001b[1;32mâ”‚\u001b[0m\n",
       "\u001b[1;32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">================================================================================\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "================================================================================\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sub-Query 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">self attention mechanism definition and purpose</span> <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m Sub-Query 1 \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m \u001b[1;37mself attention mechanism definition and purpose\u001b[0m \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Answer:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mAnswer:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Self-attention (intra-attention) relates positions within a single sequence to compute contextualized </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representations, enabling the model to represent each token with information from all other tokens in the sequence </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">[Vaswani et al., 2017, Section 2].</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mSelf-attention (intra-attention) relates positions within a single sequence to compute contextualized \u001b[0m\n",
       "\u001b[32mrepresentations, enabling the model to represent each token with information from all other tokens in the sequence \u001b[0m\n",
       "\u001b[32m[Vaswani et al., 2017, Section 2].\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Citations:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35mCitations:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  â€¢ Vaswani et al., 2017 - Attention Is All You Need; Section 2, 3.2.3, 4; </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m  â€¢ Vaswani et al., 2017 - Attention Is All You Need; Section 2, 3.2.3, 4; \u001b[0m\n",
       "\u001b[35mhttps://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Context Preview:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33mContext Preview:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single</span>\n",
       "<span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">sequence in order to compute a representation of the sequence (Vaswani et al., 2017,...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;33mSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single\u001b[0m\n",
       "\u001b[2;33msequence in order to compute a representation of the sequence (Vaswani et al., 2017,...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sub-Query 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">self attention operation within transformer architecture</span> <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m Sub-Query 2 \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m \u001b[1;37mself attention operation within transformer architecture\u001b[0m \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Answer:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mAnswer:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Transformer self-attention forms queries, keys, and values from the same input, computes attention weights (e.g., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scaled dot-product, often via multiple heads), applies those weights to values to produce context-aware outputs, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">uses masking in decoder self-attention to preserve autoregression, and includes encoderâ€“decoder attention to let </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the decoder attend to encoder outputs [Vaswani et al., 2017, Sections 3.2.3 and 4].</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mTransformer self-attention forms queries, keys, and values from the same input, computes attention weights (e.g., \u001b[0m\n",
       "\u001b[32mscaled dot-product, often via multiple heads), applies those weights to values to produce context-aware outputs, \u001b[0m\n",
       "\u001b[32muses masking in decoder self-attention to preserve autoregression, and includes encoderâ€“decoder attention to let \u001b[0m\n",
       "\u001b[32mthe decoder attend to encoder outputs [Vaswani et al., 2017, Sections 3.2.3 and 4].\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Citations:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;35mCitations:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  â€¢ Vaswani et al., 2017 - Attention Is All You Need; Section 3.2.3, 4; </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m  â€¢ Vaswani et al., 2017 - Attention Is All You Need; Section 3.2.3, 4; \u001b[0m\n",
       "\u001b[35mhttps://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Context Preview:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33mContext Preview:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">In a self-attention layer all keys, values and queries come from the same source (the previous layer) and each </span>\n",
       "<span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">position can attend to all positions in that layer; in the decoder self-attention is mask...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;33mIn a self-attention layer all keys, values and queries come from the same source (the previous layer) and each \u001b[0m\n",
       "\u001b[2;33mposition can attend to all positions in that layer; in the decoder self-attention is mask...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich.table import Table\n",
    "\n",
    "console = Console()\n",
    "\n",
    "console.print(\"\\n\" + \"=\"*80)\n",
    "console.print(Panel.fit(\"ðŸ¤– Agent Execution Summary\", style=\"bold cyan\"))\n",
    "console.print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for i, msg in enumerate(result['messages'], 1):\n",
    "    msg_content = msg.content if hasattr(msg, 'content') else str(msg)\n",
    "    if len(msg_content.strip()) == 0:\n",
    "        msg_content = \"Tool Calls: \"\n",
    "        msg_content += [tool['name'] for tool in msg.tool_calls][0]\n",
    "    header = Text(f\"Step {i}: \", style=\"bold yellow\")\n",
    "    header.append(msg_content[:50] + \"...\" if len(msg_content) > 50 else msg_content, style=\"cyan\")\n",
    "    console.print(header)\n",
    "    console.print(Text(msg_content, style=\"white dim\"))\n",
    "    console.print()\n",
    "\n",
    "# Display structured results\n",
    "console.print(\"\\n\" + \"=\"*80)\n",
    "console.print(Panel.fit(\"ðŸ“Š Retrieved Context & Answers\", style=\"bold green\"))\n",
    "console.print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if \"structured_response\" in result and \"results\" in result[\"structured_response\"]:\n",
    "    for idx, item in enumerate(result[\"structured_response\"][\"results\"], 1):\n",
    "        # Sub-query panel\n",
    "        console.print(Panel(\n",
    "            Text(item.get(\"sub_query\", \"N/A\"), style=\"bold white\"),\n",
    "            title=f\"Sub-Query {idx}\",\n",
    "            border_style=\"cyan\",\n",
    "            expand=False\n",
    "        ))\n",
    "        \n",
    "        # Synthesized answer\n",
    "        if item.get(\"synthesized_answer\"):\n",
    "            console.print(Text(\"Answer:\", style=\"bold green\"))\n",
    "            console.print(Text(item[\"synthesized_answer\"], style=\"green\"))\n",
    "        \n",
    "        # Citations\n",
    "        if item.get(\"citations\"):\n",
    "            console.print(Text(\"\\nCitations:\", style=\"bold magenta\"))\n",
    "            for citation in item[\"citations\"]:\n",
    "                console.print(Text(f\"  â€¢ {citation}\", style=\"magenta\"))\n",
    "        \n",
    "        # Context preview\n",
    "        if item.get(\"retrieved_context\"):\n",
    "            context_preview = item[\"retrieved_context\"][:200] + \"...\" if len(item[\"retrieved_context\"]) > 200 else item[\"retrieved_context\"]\n",
    "            console.print(Text(f\"\\nContext Preview:\", style=\"bold yellow\"))\n",
    "            console.print(Text(context_preview, style=\"yellow dim\"))\n",
    "        \n",
    "        console.print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6a0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
