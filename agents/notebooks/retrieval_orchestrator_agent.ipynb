{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a348d68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28c6d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sultan/Projects/citebase/agents/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from deepagents import create_deep_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc9e87",
   "metadata": {},
   "source": [
    "**RAG.**\n",
    "Define a retrieval function that the deep_agent will use as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf2f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\n",
    "TOP_K = 3\n",
    "queries = ['mechanics of scaled dot product attention',\n",
    " 'key aspects of multi head attention']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54dd9bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 23:39:02,487 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-06 23:39:02,575 - INFO - Going to convert document batch...\n",
      "2026-01-06 23:39:02,576 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2026-01-06 23:39:02,584 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-06 23:39:02,584 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-06 23:39:02,586 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-06 23:39:02,592 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-06 23:39:02,592 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-06 23:39:02,595 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-06 23:39:03,188 - INFO - Auto OCR model selected ocrmac.\n",
      "2026-01-06 23:39:03,196 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-06 23:39:03,198 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-06 23:39:03,204 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-06 23:39:03,208 - INFO - Accelerator device: 'mps'\n",
      "2026-01-06 23:39:05,101 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-06 23:39:05,105 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-06 23:39:05,108 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-06 23:39:05,626 - INFO - Accelerator device: 'mps'\n",
      "2026-01-06 23:39:06,376 - INFO - Processing document NIPS-2017-attention-is-all-you-need-Paper.pdf\n",
      "2026-01-06 23:39:19,142 - INFO - Finished converting document NIPS-2017-attention-is-all-you-need-Paper.pdf in 19.91 sec.\n",
      "/Users/sultan/Projects/citebase/agents/.venv/lib/python3.14/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2026-01-06 23:39:21,738 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "import tiktoken\n",
    "from docling_core.transforms.chunker.tokenizer.openai import OpenAITokenizer\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenizer = OpenAITokenizer(\n",
    "    tokenizer=enc,\n",
    "    max_tokens=128 * 1024,  # set to the model's context window\n",
    ")\n",
    "\n",
    "loader = DoclingLoader(file_path=FILE_PATH, chunker=HybridChunker(tokenizer=tokenizer))\n",
    "docs = loader.load()\n",
    "\n",
    "milvus_uri = str(Path(mkdtemp()) / \"vector.db\")\n",
    "\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"vectordb\",\n",
    "    connection_args={\"uri\": milvus_uri},\n",
    "    index_params={\"index_type\": \"FLAT\"},\n",
    "    drop_old=True,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "\"\"\"You must answer using ONLY the context below. Do not use outside knowledge.\n",
    "\n",
    "CONTEXT (each excerpt includes its citation tag like [source:... page:... chunk:...])\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "\n",
    "QUERY: {input}\n",
    "\n",
    "CITATION RULES\n",
    "- Every factual claim must end with a citation tag copied from the context, like: [source:XYZ page:12 chunk:5].\n",
    "- If a sentence contains multiple claims from different excerpts, include multiple citation tags at the end of that sentence.\n",
    "- Do NOT invent citation tags. Use only tags that appear in the context verbatim.\n",
    "\n",
    "Return exactly:\n",
    "1) Final answer (short, 2–6 sentences, with citations)\n",
    "2) Key points (3–7 bullets, each bullet with citations)\n",
    "3) Assumptions (or \"None\")\n",
    "If the context doesn't support the answer, say: \"Not answerable from context.\"\n",
    "\n",
    "RESPONSE:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9247c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def retrieve_from_vectorstore(queries: list[str]) -> dict:\n",
    "    \"\"\"Retrieve and generate answers from the vectorstore for a list of queries.\n",
    "    This function invokes the RAG chain for each query and returns aggregated results.\"\"\"\n",
    "    \n",
    "    resp_dict={}\n",
    "    for query in queries:\n",
    "        resp_dict[query] = rag_chain.invoke({\"input\": query})\n",
    "        \n",
    "    return resp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cf964",
   "metadata": {},
   "source": [
    "**Prepare settings for subagents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd667a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"\"\"\n",
    "Example 1 single information need\n",
    "Input query:\n",
    "How does dropout prevent overfitting in neural networks\n",
    "Output:\n",
    "{\"sub_queries\":[\"dropout regularization reducing neural network overfitting\"]}\n",
    "\n",
    "Example 2 two distinct information needs\n",
    "Input query:\n",
    "effects of microplastics on marine food webs and human health risks\n",
    "Output:\n",
    "{\"sub_queries\":[\"microplastics impacts on marine food webs\",\"human health risks from microplastic exposure\"]}\n",
    "\n",
    "Example 3 avoid over splitting broad impact\n",
    "Input query:\n",
    "What are the latest advancements in natural language processing and how do they impact machine learning models\n",
    "Output:\n",
    "{\"sub_queries\":[\"recent advancements in natural language processing\",\"impact of recent NLP advances on machine learning models\"]}\n",
    "\n",
    "Example 4 three distinct information needs\n",
    "Input query:\n",
    "role of gut microbiome in obesity and type 2 diabetes and dietary interventions\n",
    "Output:\n",
    "{\"sub_queries\":[\"gut microbiome links to obesity\",\"gut microbiome links to type 2 diabetes\",\"dietary interventions modulating gut microbiome in metabolic disease\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "query_decomposition_prompt = \"\"\"\n",
    "You are an academic query decomposition and refinement agent for vector retrieval.\n",
    "You will receive a query from a user which may contain single or multiple distinct information needs.\n",
    "\n",
    "Here are the examples which are done well:\n",
    "{examples}\n",
    "\n",
    "Goal:\n",
    "Return a list of refined sub-queries for retrieving relevant academic context from a vector database.\n",
    "\n",
    "Output (strict):\n",
    "- Return ONLY an object with key \"sub_queries\" containing a list of strings.\n",
    "- Each string must be a refined retrieval query.\n",
    "- Do NOT add any other keys or any extra text.\n",
    "\n",
    "Decomposition rules:\n",
    "- If the input expresses ONE coherent information need, return exactly 1 refined query.\n",
    "- If the input contains multiple distinct information needs different questions topics or aspects return 2 to 5 sub-queries.\n",
    "- Each sub-query must be atomic one concept or aspect only.\n",
    "- Avoid overlap no near-duplicate sub-queries.\n",
    "\n",
    "Refinement rules apply to every sub-query:\n",
    "- Preserve the user intent exactly do not change meaning.\n",
    "- Do NOT add new concepts not present or clearly implied.\n",
    "- Use precise academic or technical terminology.\n",
    "- Prefer noun phrases avoid questions and full sentences.\n",
    "- Remove filler words stopwords and conversational phrasing.\n",
    "- Length 5 to 12 words per sub-query max 12.\n",
    "- Do NOT use punctuation quotes Boolean operators or special syntax.\n",
    "- Do NOT include years author names or datasets unless explicitly present in the input.\n",
    "- Properly identify distinct aspects if the input is broad or vague.\n",
    "- Do NOT over-split sub-queries unnecessarily.\n",
    "- Try to keep related concepts together.\n",
    "\n",
    "Return the structured output now.\n",
    "\"\"\".format(examples=examples)\n",
    "\n",
    "query_decomposition_model = \"gpt-5-mini\"\n",
    "query_decomposition_description = \"Analyzes research queries and generates optimized, non-overlapping sub-queries for vector database retrieval.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e1530a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_retrieval_prompt = \"\"\"\n",
    "You are a vectorstore retrieval orchestration agent. Your primary responsibility is to retrieve academic context from a vector database using refined sub-queries.\n",
    "\n",
    "Task:\n",
    "You will receive one or more sub-queries from the query decomposition agent. Your role is to use the retrieve_from_vectorstore tool to fetch relevant academic papers, citations, and context for each sub-query.\n",
    "\n",
    "Instructions:\n",
    "1. Accept the list of sub-queries provided by the upstream agent.\n",
    "2. Call retrieve_from_vectorstore with ALL sub-queries at once to retrieve relevant documents and answers from the vector database.\n",
    "3. The tool returns a dictionary mapping each sub-query to its retrieved context and generated answer.\n",
    "4. Aggregate and structure the retrieved results, ensuring no loss of information.\n",
    "5. Present the aggregated retrieval results in a clear, hierarchical format organized by sub-query.\n",
    "\n",
    "Output format:\n",
    "- Organize results by sub-query as keys\n",
    "- Include retrieved context, citations, and synthesized answers for each sub-query\n",
    "- Preserve all citation metadata (source, page, chunk information)\n",
    "- Flag any sub-queries that returned insufficient context\n",
    "\n",
    "Important:\n",
    "- Always use retrieve_from_vectorstore to access the vector database. Do not attempt to retrieve information manually.\n",
    "- Ensure all sub-queries are passed to the tool in a single batch for efficiency.\n",
    "- If retrieval fails for any sub-query, clearly indicate which ones had issues.\n",
    "\"\"\"\n",
    "\n",
    "vectorstore_retrieval_model = \"gpt-5-nano\"\n",
    "vectorstore_retrieval_description = \"Executes batch retrieval of academic context from vector database and preserves citations and metadata for structured result aggregation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b3c35",
   "metadata": {},
   "source": [
    "**Main Deep Agent**\n",
    "- Decomposes user queries into subqueries\n",
    "- Retrieves relevant information from the vectorstore using subqueries\n",
    "- Formats and structures the retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764383f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from deepagents.middleware.subagents import SubAgentMiddleware\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"gpt-5-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9558bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "class AggregatedContext(TypedDict):\n",
    "    sub_query: Annotated[str, \"The sub-query string\"]\n",
    "    retrieved_context: Annotated[str, \"The retrieved context string\"]\n",
    "    citations: Annotated[List[str], \"List of citation identifiers\"]\n",
    "    synthesized_answer: Annotated[str, \"The synthesized answer string\"]\n",
    "    \n",
    "class AggregatedContextList(TypedDict):\n",
    "    results: Annotated[List[AggregatedContext], \"List of aggregated context for each sub-query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6509636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "You are the main retrieval orchestrator for academic question answering. You coordinate sub-agents for query decomposition and vectorstore retrieval, then deliver a concise, citation-backed response.\n",
    "\n",
    "Workflow:\n",
    "1) Send the user query to query_decomposition_subagent. Expect an object with key sub_queries.\n",
    "2) If decomposition fails or returns no sub-queries, fall back to a single sub-query equal to the original user query.\n",
    "3) Send the full sub_queries list to vectorstore_retrieval_subagent (retrieve_from_vectorstore tool) in one batch. Expect a dictionary mapping each sub-query to retrieved context and an answer with citations.\n",
    "4) Aggregate all sub-query answers into a single, coherent response.\n",
    "\n",
    "Response format (strict):\n",
    "- Final answer: 2–6 sentences, each factual claim ends with citations from the retrieved context.\n",
    "- Key points: 3–7 bullets, each bullet ends with citations.\n",
    "- Assumptions: list assumptions made, or \"None\".\n",
    "\n",
    "Rules:\n",
    "- Never invent citations; only use those provided by the retrieval results.\n",
    "- If any sub-query returned insufficient context, say so explicitly for that sub-query.\n",
    "- If no context supports an answer, reply: \"Not answerable from context.\".\n",
    "- Keep wording concise and academic; avoid markdown tables and superfluous formatting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02e20205",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=PROMPT,\n",
    "    middleware=[\n",
    "        SubAgentMiddleware(\n",
    "            default_model=\"gpt-4o\",\n",
    "            default_tools=[],\n",
    "            subagents=[\n",
    "                {\n",
    "                    \"name\": \"query_decomposition_subagent\",\n",
    "                    \"description\": query_decomposition_description,\n",
    "                    \"system_prompt\": query_decomposition_prompt,\n",
    "                    \"model\": query_decomposition_model,\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"vectorstore_retrieval_subagent\",\n",
    "                    \"description\": vectorstore_retrieval_description,\n",
    "                    \"system_prompt\": vectorstore_retrieval_prompt,\n",
    "                    \"tools\": [retrieve_from_vectorstore],\n",
    "                    \"model\": vectorstore_retrieval_model,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    "    response_format=AggregatedContextList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0493c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 00:22:22,912 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:22:24,956 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:22:27,413 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:22:38,678 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:22:44,411 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:22:48,829 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:22:50,554 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:22:57,314 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:22:58,337 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:23:04,276 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:23:05,504 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:23:10,421 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:23:12,009 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:23:18,510 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:23:19,125 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:23:25,165 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:23:26,442 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:23:30,695 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:25:07,161 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-07 00:25:53,691 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the self attention mechanism and how does it work in transformer models?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69ebdfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sub_query': 'self attention mechanism definition',\n",
       "  'retrieved_context': 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. [cite:463398441214279686]\\nIn a self-attention layer all of the keys, values, and queries come from the same place, specifically the output of the previous layer, allowing each position to attend to all positions in the previous layer. [cite:463398441214279690]\\nThe Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. [cite:463398441214279686]',\n",
       "  'citations': ['463398441214279686', '463398441214279690'],\n",
       "  'synthesized_answer': 'Self-attention, also called intra-attention, relates different positions within a single sequence to compute contextualized representations; queries, keys, and values are all derived from the same input representation. [463398441214279686] [463398441214279690]'},\n",
       " {'sub_query': 'mathematical formulation of self attention queries keys values',\n",
       "  'retrieved_context': 'An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. [cite:463398441214279690]\\nIn Scaled Dot-Product Attention, the dot products of the query with all keys are computed, divided by the square root of the dimension of the keys, and then a softmax function is applied to obtain the weights on the values. [cite:463398441214279688]\\nThe dot-product attention variant is typically faster and more space-efficient in practice, with scaling (1/√d_k) used to keep gradients in a healthy range for larger d_k. [cite:463398441214279688]',\n",
       "  'citations': ['463398441214279690', '463398441214279688'],\n",
       "  'synthesized_answer': 'Self-attention maps each query and a set of key–value pairs to an output by computing compatibility scores between the query and each key and then forming a weighted sum of the values; vectors are used for queries, keys, and values. [463398441214279690] [463398441214279688]'},\n",
       " {'sub_query': 'scaled dot-product in self attention',\n",
       "  'retrieved_context': 'Scaled Dot-Product Attention: dot products with keys are computed, divided by √d_k, and a softmax is applied to obtain weights on the values. [cite:463398441214279688]\\nTo counteract large dot-product magnitudes, the dot products are scaled by 1/√d_k. [cite:463398441214279688]\\nDot-product attention is faster and more space-efficient in practice since it can be implemented using highly optimized matrix multiplication code. [cite:463398441214279688]',\n",
       "  'citations': ['463398441214279688'],\n",
       "  'synthesized_answer': 'Scaled dot-product attention computes QKᵀ, scales the scores by 1/√d_k, and applies softmax to produce attention weights; the scaling stabilizes gradients and numerical behavior when d_k is large. [463398441214279688]'},\n",
       " {'sub_query': 'multi-head attention in transformers',\n",
       "  'retrieved_context': 'Instead of a single attention function, multi-head attention linearly projects queries, keys, and values h times with different learned projections to d_k, d_k, and d_v respectively, performs attention in parallel on each projection, and then concatenates and projects the outputs to form the final values. [cite:463398441214279691]\\nWe employ h = 8 parallel attention heads, and set d_k = d_v = d_model/h = 64. [cite:463398441214279691]\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. [cite:463398441214279691]\\nThe Transformer uses multi-head attention in encoder-decoder attention layers, encoder self-attention layers, and decoder self-attention layers. [cite:463398441214279690]',\n",
       "  'citations': ['463398441214279691', '463398441214279690'],\n",
       "  'synthesized_answer': 'Multi-head attention projects Q, K, V several times with different learned linear maps, performs attention in parallel across the heads, concatenates the head outputs, and applies a final linear projection; this lets the model attend to different representation subspaces simultaneously. [463398441214279691] [463398441214279690]'},\n",
       " {'sub_query': 'role of self attention in transformer architecture',\n",
       "  'retrieved_context': 'Self-attention connects all positions with a constant number of sequential operations, enabling learning of long-range dependencies with a constant per-layer cost. [cite:463398441214279694]\\nThe Transformer uses self-attention in both the encoder and the decoder; this allows each position to attend to all positions in the previous layer. [cite:463398441214279690]\\nSelf-attention can lead to more interpretable models, with attention heads learning to perform different tasks related to syntax and semantics. [cite:463398441214279694]',\n",
       "  'citations': ['463398441214279694', '463398441214279690'],\n",
       "  'synthesized_answer': 'Self-attention is central to the Transformer encoder and decoder, connecting all positions so each can attend to every other position and thereby capture long-range dependencies with a constant number of sequential operations per layer. [463398441214279694] [463398441214279690]'},\n",
       " {'sub_query': 'integration of positional encodings in transformers',\n",
       "  'retrieved_context': 'Positional encodings are added to the input embeddings because the model contains no recurrence or convolution, enabling it to use the order of the sequence. [cite:463398441214279685]\\nThe positional encodings have the same dimension as the embeddings and are summed with the embeddings at the bottoms of the encoder and decoder stacks. [cite:463398441214279685]\\nWe use sinusoidal positional encodings with frequencies that form a geometric progression; learned positional embeddings were tested and yielded nearly identical results. [cite:463398441214279685]',\n",
       "  'citations': ['463398441214279685'],\n",
       "  'synthesized_answer': 'Positional encodings (sinusoidal or learned) are added to input embeddings at the bottom of the encoder and decoder to provide explicit sequence order information, since the Transformer has no built-in recurrence or convolution. [463398441214279685]'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"structured_response\"]['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde49c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
